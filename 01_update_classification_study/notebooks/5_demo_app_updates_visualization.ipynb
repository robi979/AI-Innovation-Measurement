{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "In this notebook, we generate all tables and figures based on the results of demonstration study 1, for both the main paper and the online appendix. This includes data filtering, statistical summaries, and all visualization scripts. All output files are saved to the `paper_visuals` and `output_data` directories."
   ],
   "id": "404cb591d2753031"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Imports\n",
    " See `requirements.txt` for full dependency versions"
   ],
   "id": "dd85067251d6d8f2"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, cohen_kappa_score\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Global Variables, Paths, and Settings",
   "id": "6fdec05fefa491f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define Demo Study path\n",
    "DEMO_PATH   = os.path.abspath(os.path.join(\"..\"))\n",
    "\n",
    "# Define relevant paths\n",
    "OUT_DIR     = os.path.join(DEMO_PATH, \"output_data\")\n",
    "LLM_PATH    = os.path.join(OUT_DIR, \"validation_with_model_preds_LLM_cleaned.csv\")\n",
    "MODELS_PATH = os.path.join(OUT_DIR, \"validation_with_model_preds_NLP.csv\")\n",
    "LITERATURE_PATH = os.path.join(OUT_DIR, \"validation_literature_classification.csv\")\n",
    "VISUAL_DIR  = os.path.join(DEMO_PATH, \"paper_visuals\")"
   ],
   "id": "2e5f7946e92d3900",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pre-Processing\n",
    "In this section, we define regular expressions and helper functions for parsing model output columns. This includes extracting metadata (e.g., data type, size, classifier/model, and vectorizer) for classical ML, CNN, and PLM models, as well as standardizing column names and processing LLM results. These routines are used to prepare the results tables and figures for the paper and online appendix."
   ],
   "id": "b6d506ba5c73a3e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define constants for parsing model output column names and metadata extraction\n",
    "DATA_TYPES   = ['real', 'equal']\n",
    "DATA_SIZES   = ['2000','1000','500','250','100']\n",
    "CLASSIFIERS  = ['RandomForest','KNeighbors','LogisticRegression',\n",
    "                'NaiveBayes','SVM','XGBoost']\n",
    "CNN_MODELS   = ['TextCNN42B']\n",
    "PLM_MODELS   = ['roberta','bert','electra','xlnet']\n",
    "GLOVE_VECTORIZERS = [\"6B-100d\",\"6B-300d\",\"42B-300d\",\"840B-300d\"]\n",
    "\n",
    "# Pre-compiled regex patterns to match output columns for each model family\n",
    "ML_PATTERN    = re.compile(\n",
    "    rf'^(.+?)__({\"|\".join(DATA_TYPES)})__({\"|\".join(DATA_SIZES)})__'\n",
    "    rf'({\"|\".join(CLASSIFIERS)})_pred$', re.I)\n",
    "CNN_PATTERN   = re.compile(\n",
    "    rf'^({\"|\".join(CNN_MODELS)})__({\"|\".join(DATA_TYPES)})__'\n",
    "    rf'({\"|\".join(DATA_SIZES)})_pred$', re.I)\n",
    "PLM_PATTERN   = re.compile(\n",
    "    rf'^({\"|\".join(PLM_MODELS)})_({\"|\".join(DATA_TYPES)})_('\n",
    "    rf'{\"|\".join(DATA_SIZES)})_pred$', re.I)\n",
    "GLOVE_PATTERN = re.compile(\n",
    "    rf'^({\"|\".join(GLOVE_VECTORIZERS)})_({\"|\".join(DATA_TYPES)})_('\n",
    "    rf'{\"|\".join(DATA_SIZES)})__({\"|\".join(CLASSIFIERS)})_pred$', re.I)\n",
    "\n",
    "# Define known LLM and other model keys for later identification and provider mapping\n",
    "MODEL_KEYS = sorted([\n",
    "    # OpenAI\n",
    "    \"gpt_4_1_nano\", \"gpt_4_1_mini\", \"gpt_4_1\",\n",
    "    # Mistral\n",
    "    \"mistral_large\", \"mistral_medium\", \"mistral_small\",\n",
    "    \"open_mistral_nemo\", \"ministral_8b\", \"ministral_3b\",\n",
    "    # Anthropic\n",
    "    \"4_sonnet\", \"sonnet_4\",\n",
    "    \"3_5_haiku\", \"3_haiku\",\n",
    "    \"3_7_sonnet\", \"3_5_sonnet\",\n",
    "    \"3_opus\", \"opus_4\",\n",
    "    # OpenAI (legacy keys)\n",
    "    \"gpt_4o_mini\", \"gpt_4o\",\n",
    "    \"gpt_3_5_turbo_0125\",\n",
    "    \"o4_mini_2025_04_16\", \"o3_2025_04_16\",\n",
    "], key=len, reverse=True)\n",
    "\n",
    "# Mapping model keys to their provider (OpenAI, Mistral, Anthropic)\n",
    "PROVIDER = {\n",
    "    **{k:\"OpenAI\"   for k in MODEL_KEYS if k.startswith(\"gpt\") or k.startswith(\"o\")},\n",
    "    **{k:\"Mistral\"  for k in MODEL_KEYS if \"mistral\" in k or \"ministral\" in k},\n",
    "    **{k:\"Anthropic\"for k in MODEL_KEYS if k.endswith((\"haiku\",\"sonnet\",\"opus\"))},\n",
    "}"
   ],
   "id": "116a26562fb7b8c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Helper functions for column and prediction parsing",
   "id": "4e0197003175c0de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Model metadata parsing helpers\n",
    "def parse_ml_info(col):\n",
    "    \"\"\"\n",
    "    Parses ML/CNN/PLM column names to extract model metadata.\n",
    "    Returns a dictionary with keys: model, category, vectorizer, distribution, and size\n",
    "    if a known naming pattern is matched; otherwise returns None.\n",
    "    \"\"\"\n",
    "    m = GLOVE_PATTERN.match(col)\n",
    "    if m:\n",
    "        vec, dist, size, clf = m.groups()\n",
    "        return dict(model=clf, category=\"ML\", vectorizer=vec, distribution=dist, size=int(size))\n",
    "    m = ML_PATTERN.match(col)\n",
    "    if m:\n",
    "        vec, dist, size, clf = m.groups()\n",
    "        return dict(model=clf, category=\"ML\", vectorizer=vec, distribution=dist, size=int(size))\n",
    "    m = CNN_PATTERN.match(col)\n",
    "    if m:\n",
    "        model, dist, size = m.groups()\n",
    "        return dict(model=model, category=\"CNN\", vectorizer=None, distribution=dist, size=int(size))\n",
    "    m = PLM_PATTERN.match(col)\n",
    "    if m:\n",
    "        model, dist, size = m.groups()\n",
    "        return dict(model=model, category=\"PLM\", vectorizer=None, distribution=dist, size=int(size))\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_llm_info(prefix):\n",
    "    \"\"\"\n",
    "    Parses LLM column prefixes to extract metadata such as model, provider, prompt type,\n",
    "    temperature, fine-tuning status, distribution, and size.\n",
    "    Returns a dictionary with standardized metadata keys if parsing succeeds; otherwise None.\n",
    "    \"\"\"\n",
    "    parts = prefix.split(\"__\")\n",
    "    if len(parts) != 3: return None\n",
    "    core, prompt_type, temp_str = parts\n",
    "    fine_tuned = int(core.startswith(\"ft_\"))\n",
    "    if fine_tuned: core = core[3:]\n",
    "    model = next((mk for mk in MODEL_KEYS if mk in core), None)\n",
    "    provider = PROVIDER.get(model)\n",
    "    m_ds = re.search(rf\"_(real|equal)_({'|'.join(DATA_SIZES)})\", core)\n",
    "    distribution, size = (m_ds.group(1), int(m_ds.group(2))) if m_ds else (None, None)\n",
    "    temperature = None\n",
    "    if temp_str.startswith(\"t\"):\n",
    "        try: temperature = int(temp_str[1:]) / 10.0\n",
    "        except: temperature = None\n",
    "    return dict(\n",
    "        model=model or core, category=\"LLM\", provider=provider, prompt_type=prompt_type,\n",
    "        temperature=temperature, fine_tuned=fine_tuned, distribution=distribution, size=size,\n",
    "        vectorizer=None\n",
    "    )\n",
    "\n",
    "# Metric computation helpers\n",
    "def innovation_dummy(labels: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Converts a multiclass label series into a binary target for innovation tasks:\n",
    "    labels <= 2 are mapped to 1 (positive class), others to 0 (negative class).\n",
    "    \"\"\"\n",
    "    return (labels <= 2).astype(int)\n",
    "\n",
    "def compute_binary_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes binary classification metrics: macro F1, precision, recall, and accuracy.\n",
    "    Returns a dictionary of metric names to scores.\n",
    "    \"\"\"\n",
    "    return dict(\n",
    "        f1_macro=f1_score(y_true, y_pred, average=\"binary\", zero_division=0),\n",
    "        precision=precision_score(y_true, y_pred, average=\"binary\", zero_division=0),\n",
    "        recall=recall_score(y_true, y_pred, average=\"binary\", zero_division=0),\n",
    "        accuracy=accuracy_score(y_true, y_pred)\n",
    "    )\n",
    "\n",
    "def compute_multiclass_f1(y_true, y_pred, labels=range(1,8)):\n",
    "    \"\"\"\n",
    "    Computes per-class F1, macro F1, and weighted F1 for multiclass predictions.\n",
    "    Returns a dictionary of all scores.\n",
    "    \"\"\"\n",
    "    f1s = f1_score(y_true, y_pred, labels=list(labels), average=None, zero_division=0)\n",
    "    out = {f'f1_{lab}': f1s[lab-1] for lab in labels}\n",
    "    out['f1_macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    out['f1_weighted'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    return out\n",
    "\n",
    "def compute_fleiss_kappa_binary(preds_df):\n",
    "    \"\"\"\n",
    "    Computes Fleiss' kappa statistic for binary predictions (multiple raters).\n",
    "    Returns the kappa value.\n",
    "    \"\"\"\n",
    "    counts = np.stack([(1 - preds_df.values).sum(1), preds_df.values.sum(1)], axis=1)\n",
    "    return fleiss_kappa(counts)\n",
    "\n",
    "def compute_fleiss_kappa_multiclass(preds_df, labels=range(1,8)):\n",
    "    \"\"\"\n",
    "    Computes Fleiss' kappa statistic for multiclass predictions (multiple raters).\n",
    "    Returns the kappa value.\n",
    "    \"\"\"\n",
    "    arr = preds_df.values\n",
    "    countmat = []\n",
    "    for row in arr:\n",
    "        countmat.append([(row==lab).sum() for lab in labels])\n",
    "    return fleiss_kappa(np.array(countmat))"
   ],
   "id": "550ddef8ecb030eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Main function for compiling results DataFrame for all models",
   "id": "485d872cbdb2917f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_results_table(df_ml, df_llm, task=\"innovation\"):\n",
    "    \"\"\"\n",
    "    Build a tidy DataFrame summarizing all multilabel metrics, consistency, and model metadata\n",
    "    for all predictors (ML, CNN, PLM, LLM). Used for generating result tables and figures.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "\n",
    "    # Ground-truth label arrays\n",
    "    TRUTH_COL = \"update_classification\"\n",
    "    label_fn = innovation_dummy if task==\"innovation\" else (lambda x: x.astype(int))\n",
    "\n",
    "    # Literature approaches\n",
    "    if task == \"innovation\":\n",
    "        literature_cols = ['first_digit', 'second_digit', 'KF23_innovation', 'AK23_innovation']\n",
    "        y_true_lit = label_fn(df_ml[TRUTH_COL])\n",
    "        for col in literature_cols:\n",
    "            if col not in df_ml.columns:\n",
    "                continue\n",
    "            try:\n",
    "                y_pred = df_ml[col].astype(int)\n",
    "            except Exception:\n",
    "                continue\n",
    "            metrics = compute_binary_metrics(y_true_lit, y_pred)\n",
    "            rec = {\n",
    "                \"raw_name\": col,\n",
    "                \"raw_prefix\": None,\n",
    "                \"model\": col,\n",
    "                \"category\": \"Literature\",\n",
    "                \"provider\": None,\n",
    "                \"prompt_type\": None,\n",
    "                \"temperature\": None,\n",
    "                \"fine_tuned\": None,\n",
    "                \"distribution\": None,\n",
    "                \"size\": None,\n",
    "                \"vectorizer\": None,\n",
    "                **metrics,\n",
    "                \"fleiss_kappa\": np.nan,\n",
    "                \"mean_cohen_kappa\": np.nan,\n",
    "                \"consistency_all_runs\": np.nan\n",
    "            }\n",
    "            records.append(rec)\n",
    "\n",
    "    # ML / CNN / PLM models\n",
    "    y_true_ml = label_fn(df_ml[TRUTH_COL])\n",
    "    for col in df_ml.columns:\n",
    "        info = parse_ml_info(col)\n",
    "        if info is None: continue\n",
    "        # Binary: predictions in 0–6, shift to 1–7, then dummy\n",
    "        try:\n",
    "            y_pred_raw = df_ml[col].astype(int) + 1 if info['category'] in ('ML','CNN','PLM') else df_ml[col].astype(int)\n",
    "        except Exception: continue\n",
    "        y_pred = label_fn(y_pred_raw)\n",
    "        if task==\"innovation\":\n",
    "            metrics = compute_binary_metrics(y_true_ml, y_pred)\n",
    "        else:\n",
    "            metrics = compute_multiclass_f1(y_true_ml, y_pred)\n",
    "        rec = {\n",
    "            \"raw_name\": col, \"raw_prefix\": None, **info,\n",
    "            \"provider\": info.get(\"provider\", None),\n",
    "            \"prompt_type\": info.get(\"prompt_type\", None),\n",
    "            \"temperature\": info.get(\"temperature\", None),\n",
    "            \"fine_tuned\": info.get(\"fine_tuned\", None),\n",
    "            **metrics,\n",
    "            \"fleiss_kappa\": np.nan, \"mean_cohen_kappa\": np.nan, \"consistency_all_runs\": np.nan\n",
    "        }\n",
    "        records.append(rec)\n",
    "\n",
    "    # LLMs\n",
    "    y_true_llm = label_fn(df_llm[TRUTH_COL])\n",
    "    all_llm_cols = [c for c in df_llm.columns if (\"__run\" in c) or re.search(r\"__r\\d+$\", c)]\n",
    "    prefix_to_runs = {}\n",
    "    for col in all_llm_cols:\n",
    "        prefix = col.rsplit(\"__run\", 1)[0] if \"__run\" in col else col.rsplit(\"__r\", 1)[0]\n",
    "        prefix_to_runs.setdefault(prefix, []).append(col)\n",
    "    for prefix, run_cols in prefix_to_runs.items():\n",
    "        info = parse_llm_info(prefix)\n",
    "        if not info: continue\n",
    "        run1_col = next((c for c in run_cols if c.endswith(\"__run1\") or c.endswith(\"__r1\")), None)\n",
    "        if not run1_col: continue\n",
    "        try:\n",
    "            y_pred_run1 = label_fn(df_llm[run1_col].astype(int))\n",
    "        except Exception: continue\n",
    "        if task==\"innovation\":\n",
    "            metrics = compute_binary_metrics(y_true_llm, y_pred_run1)\n",
    "        else:\n",
    "            metrics = compute_multiclass_f1(y_true_llm, y_pred_run1)\n",
    "        fleiss_k = mean_ck = consistency = np.nan\n",
    "        try:\n",
    "            preds_df = pd.DataFrame({\n",
    "                rc: label_fn(df_llm[rc].astype(int))\n",
    "                for rc in run_cols if rc in df_llm\n",
    "            })\n",
    "            preds_df = preds_df.dropna(axis=0, how='any')\n",
    "            # Only if there are >=2 runs and non-empty\n",
    "            if preds_df.shape[1] > 1 and preds_df.shape[0] > 0:\n",
    "                try:\n",
    "                    if task==\"innovation\":\n",
    "                        fleiss_k = compute_fleiss_kappa_binary(preds_df)\n",
    "                    else:\n",
    "                        fleiss_k = compute_fleiss_kappa_multiclass(preds_df)\n",
    "                except Exception:\n",
    "                    fleiss_k = np.nan\n",
    "                try:\n",
    "                    cohens = [\n",
    "                        cohen_kappa_score(preds_df[a], preds_df[b])\n",
    "                        for i, a in enumerate(preds_df.columns)\n",
    "                        for j, b in enumerate(preds_df.columns) if j > i\n",
    "                    ]\n",
    "                    mean_ck = float(np.mean(cohens)) if cohens else np.nan\n",
    "                except Exception:\n",
    "                    mean_ck = np.nan\n",
    "                try:\n",
    "                    consistency = (preds_df.nunique(axis=1) == 1).mean()\n",
    "                except Exception:\n",
    "                    consistency = np.nan\n",
    "        except Exception:\n",
    "            fleiss_k = mean_ck = consistency = np.nan\n",
    "        rec = {\n",
    "            \"raw_name\": None, \"raw_prefix\": prefix, **info, **metrics,\n",
    "            \"fleiss_kappa\": fleiss_k,\n",
    "            \"mean_cohen_kappa\": mean_ck,\n",
    "            \"consistency_all_runs\": consistency\n",
    "        }\n",
    "        records.append(rec)\n",
    "    return pd.DataFrame.from_records(records)"
   ],
   "id": "47fa95034fd348fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Load Model Predictions and Compile Results Table\n",
    "We load the output prediction files for classical models (`MODELS_PATH`) and LLMs (`LLM_PATH`). We standardize column names, parse predictions, and compile a tidy results table containing all relevant evaluation metrics and metadata for each predictor. Processed results are saved in the `output_data` directory for downstream table and figure generation."
   ],
   "id": "e49c814688b99115"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load prediction files\n",
    "df_models_raw = pd.read_csv(MODELS_PATH, low_memory=False)\n",
    "df_llm_raw    = pd.read_csv(LLM_PATH,    low_memory=False)\n",
    "df_lit        = pd.read_csv(LITERATURE_PATH, low_memory=False)\n",
    "\n",
    "# Merge models and literature df for processing\n",
    "literature_cols = ['first_digit', 'second_digit', 'KF23_innovation', 'AK23_innovation']\n",
    "cols_to_add = [c for c in literature_cols if c in df_lit.columns and c not in df_models_raw.columns]\n",
    "df_models_raw = pd.concat(\n",
    "    [df_models_raw.reset_index(drop=True), df_lit[cols_to_add].reset_index(drop=True)],\n",
    "    axis=1\n",
    ") if cols_to_add else df_models_raw\n",
    "\n",
    "# Compile results for all models and LLMs\n",
    "innovation_binary_df = build_results_table(df_models_raw, df_llm_raw, task=\"innovation\")\n",
    "innovation_binary_df.to_csv(os.path.join(OUT_DIR, \"performance_updates_innovation.csv\"), index=False)\n",
    "\n",
    "classification_df = build_results_table(df_models_raw, df_llm_raw, task=\"classification\")\n",
    "classification_df.to_csv(os.path.join(OUT_DIR, \"performance_updates_label-specific.csv\"), index=False)"
   ],
   "id": "bb4470d4a431d8c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Visuals\n",
    "In this section, we generate all tables (Tables 2, 3, 5, and 6) and figures (Figures 6, 7, and C1) for the paper and online appendix, based on the processed results."
   ],
   "id": "8e9213f9ad4c466b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the processed results table from the previous steps\n",
    "innovation_binary_df = pd.read_csv(os.path.join(OUT_DIR, \"performance_updates_innovation.csv\"))\n",
    "classification_df = pd.read_csv(os.path.join(OUT_DIR, \"performance_updates_label-specific.csv\"))"
   ],
   "id": "be7c18e21932acbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tables",
   "id": "90a893ec6a1f0d42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define model groupings for later filtering\n",
    "LITERATURE_APPROACHES = ['first_digit', 'second_digit', 'KF23_innovation', 'AK23_innovation']\n",
    "CLASSICAL_MODELS = [\n",
    "    \"KNeighbors\",\"LogisticRegression\",\"NaiveBayes\",\n",
    "    \"RandomForest\",\"SVM\",\"XGBoost\",\n",
    "]\n",
    "CNN_MODELS = [\"TextCNN42B\"]\n",
    "PLM_MODELS = [\"bert\",\"electra\",\"roberta\",\"xlnet\"]\n",
    "LLM_BASE_MODELS = [\n",
    "    \"3_5_haiku\",\"sonnet_4\",\"4_sonnet\",\n",
    "    \"mistral_small\",\"mistral_large\",\n",
    "    \"gpt_4_1_nano\",\"gpt_4_1_mini\",\"gpt_4_1\",\n",
    "]\n",
    "GPT4_FAMILY = [\"gpt_4_1\",\"gpt_4_1_mini\",\"gpt_4_1_nano\"]\n",
    "\n",
    "ALL_COMPARATORS = (\n",
    "    CLASSICAL_MODELS +\n",
    "    CNN_MODELS +\n",
    "    PLM_MODELS +\n",
    "    LLM_BASE_MODELS\n",
    ")\n",
    "\n",
    "# Define Table Output Columns (2 and the others are different)\n",
    "\n",
    "# For binary: summary\n",
    "BINARY_COLS = [\n",
    "    \"model\", \"category\", \"provider\", \"fine_tuned\", \"temperature\", \"distribution\", \"size\", \"prompt_type\", \"vectorizer\",\n",
    "    \"f1_macro\", \"precision\", \"recall\", \"accuracy\", \"fleiss_kappa\", \"mean_cohen_kappa\", \"consistency_all_runs\"\n",
    "]\n",
    "\n",
    "# For multiclass: per-label F1 etc.\n",
    "MULTICLASS_COLS = [\n",
    "    \"model\", \"category\", \"provider\", \"fine_tuned\", \"temperature\", \"distribution\", \"size\", \"prompt_type\", \"vectorizer\",\n",
    "    \"f1_1\", \"f1_2\", \"f1_3\", \"f1_4\", \"f1_5\", \"f1_6\", \"f1_7\",\n",
    "    \"f1_macro\", \"f1_weighted\", \"fleiss_kappa\", \"mean_cohen_kappa\", \"consistency_all_runs\"\n",
    "]\n",
    "\n",
    "# Table specs for each output table: filter logic and model lists\n",
    "TABLE_SPECS = {\n",
    "    \"table2\": { # Innovation binary, all comparators, representative setting\n",
    "        \"models\" : ALL_COMPARATORS + LITERATURE_APPROACHES,\n",
    "        \"filters\": dict(\n",
    "            temperature =(0.0, np.nan),\n",
    "            distribution=(\"real\", np.nan),\n",
    "            size        =(2000, np.nan),\n",
    "            prompt_type =(\"default\", np.nan),\n",
    "            vectorizer  =(\"tfidf\",  np.nan),\n",
    "        ),\n",
    "        \"task\": \"innovation\"\n",
    "    },\n",
    "\n",
    "    \"table3\": { # Label-specific, all comparators, representative setting\n",
    "        \"models\" : ALL_COMPARATORS,\n",
    "        \"filters\": dict(\n",
    "            temperature =(0.0, np.nan),\n",
    "            distribution=(\"real\", np.nan),\n",
    "            size        =(2000, np.nan),\n",
    "            prompt_type =(\"default\", np.nan),\n",
    "            vectorizer  =(\"tfidf\",  np.nan),\n",
    "        ),\n",
    "        \"task\": \"classification\"\n",
    "    },\n",
    "\n",
    "    \"table5\": { # Label-specific, all LLMs, representative setting\n",
    "        \"models\" : None,   # all LLMs\n",
    "        \"filters\": dict(\n",
    "            category    =\"LLM\",\n",
    "            temperature =0.0,\n",
    "            distribution=(\"real\", np.nan),\n",
    "            size        =(2000, np.nan),\n",
    "            prompt_type =\"default\",\n",
    "        ),\n",
    "        \"task\": \"classification\"\n",
    "    },\n",
    "\n",
    "    \"table6\": { # Label-specific, only GPT-4 family models, representative setting\n",
    "        \"models\" : GPT4_FAMILY,\n",
    "        \"filters\": dict(\n",
    "            temperature =0.0,\n",
    "            distribution=(\"real\", np.nan),\n",
    "            size        =(2000, np.nan),\n",
    "        ),\n",
    "        \"task\": \"classification\"\n",
    "    },\n",
    "}"
   ],
   "id": "2a47967e6264bbb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Function to construct tables based on filters and models",
   "id": "4b5d9c2a993bd810"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def make_table(df, columns, models=None, **filters):\n",
    "    \"\"\"\n",
    "    Construct a filtered table based on provided models and column filters.\n",
    "    - models: list or None. If list, keep only those models.\n",
    "    - filters: key-value pairs; if tuple/list, allow any (or NaN).\n",
    "    - columns: only columns present in both the input list and the DataFrame are kept.\n",
    "    \"\"\"\n",
    "    mask = pd.Series(True, index=df.index)\n",
    "    if models is not None:\n",
    "        mask &= df[\"model\"].isin(models)\n",
    "    for col, val in filters.items():\n",
    "        if isinstance(val, (list, tuple, set)):\n",
    "            mask &= (df[col].isin(val) | df[col].isna())\n",
    "        else:\n",
    "            mask &= ((df[col] == val) | df[col].isna())\n",
    "    cols_keep = [c for c in columns if c in df.columns]\n",
    "    return df.loc[mask, cols_keep].copy()"
   ],
   "id": "89a1e4a17b83279f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Build and export all tables",
   "id": "ec6ba1b40fc5ac9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for tname, spec in TABLE_SPECS.items():\n",
    "    if spec[\"task\"] == \"innovation\":\n",
    "        df_use = innovation_binary_df\n",
    "        colspec = BINARY_COLS\n",
    "    else:\n",
    "        df_use = classification_df\n",
    "        colspec = MULTICLASS_COLS\n",
    "\n",
    "    table_df = make_table(\n",
    "        df_use,\n",
    "        columns=colspec,\n",
    "        models=spec.get(\"models\"),\n",
    "        **spec.get(\"filters\", {})\n",
    "    )\n",
    "    out_path = os.path.join(VISUAL_DIR, f\"{tname}_updates.xlsx\")\n",
    "    table_df.to_excel(out_path, index=False)"
   ],
   "id": "bb49b453e85b77f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Figures",
   "id": "68075e94be027b6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set global matplotlib visual style for figures\n",
    "plt.rcParams.update({\n",
    "    # Typography\n",
    "    \"font.family\"      : \"Times New Roman\",\n",
    "    \"font.size\"        : 11,\n",
    "    \"axes.titlesize\"   : 14,\n",
    "    \"axes.labelsize\"   : 12,\n",
    "    \"xtick.labelsize\"  : 10,\n",
    "    \"ytick.labelsize\"  : 10,\n",
    "    \"legend.fontsize\"  : 8,\n",
    "    # Grid\n",
    "    \"grid.linestyle\"   : \"--\",\n",
    "    \"grid.linewidth\"   : 0.5,\n",
    "    # Figure saving defaults\n",
    "    \"savefig.dpi\"      : 300,\n",
    "    \"savefig.bbox\"     : \"tight\",\n",
    "})\n",
    "\n",
    "# Define color and marker palettes for LLM and non-LLM models\n",
    "STYLE_LLM = {\n",
    "    \"gpt_4_1\":       {\"color\": \"#08306B\", \"marker\": \"o\", \"label\": \"GPT-4.1\"},\n",
    "    \"gpt_4_1_mini\":  {\"color\": \"#2171B5\", \"marker\": \"s\", \"label\": \"GPT-4.1 Mini\"},\n",
    "    \"gpt_4_1_nano\":  {\"color\": \"#6BAED6\", \"marker\": \"^\", \"label\": \"GPT-4.1 Nano\"},\n",
    "    \"mistral_large\": {\"color\": \"#006D2C\", \"marker\": \"D\", \"label\": \"Mistral Large\"},\n",
    "    \"mistral_small\": {\"color\": \"#74C476\", \"marker\": \"p\", \"label\": \"Mistral Small\"},\n",
    "    \"3_5_haiku\":     {\"color\": \"#A50F15\", \"marker\": \"X\", \"label\": \"Claude Haiku 3.5\"},\n",
    "    \"sonnet_4\":      {\"color\": \"#FB6A4A\", \"marker\": \"*\", \"label\": \"Claude Sonnet 4\"},\n",
    "}\n",
    "STYLE_NON = {\n",
    "    \"RandomForest\":       {\"color\": \"#8B0000\", \"marker\": \"o\", \"label\": \"Random Forest\"},\n",
    "    \"KNeighbors\":         {\"color\": \"#B22222\", \"marker\": \"s\", \"label\": \"K-Nearest Neighbor\"},\n",
    "    \"LogisticRegression\": {\"color\": \"#DC143C\", \"marker\": \"^\", \"label\": \"Logistic Regression\"},\n",
    "    \"NaiveBayes\":         {\"color\": \"#FF6347\", \"marker\": \"D\", \"label\": \"Naïve Bayes\"},\n",
    "    \"SVM\":                {\"color\": \"#FA8072\", \"marker\": \"v\", \"label\": \"SVM\"},\n",
    "    \"XGBoost\":            {\"color\": \"#FF4500\", \"marker\": \"p\", \"label\": \"XGBoost\"},\n",
    "    \"TextCNN42B\":         {\"color\": \"#FFD700\", \"marker\": \"h\", \"label\": \"CNN\"},\n",
    "    \"bert\":               {\"color\": \"#2F4F4F\", \"marker\": \"X\", \"label\": \"BERT\"},\n",
    "    \"electra\":            {\"color\": \"#696969\", \"marker\": \"*\", \"label\": \"ELECTRA\"},\n",
    "    \"roberta\":            {\"color\": \"#A9A9A9\", \"marker\": \"+\", \"label\": \"RoBERTa\"},\n",
    "    \"xlnet\":              {\"color\": \"#D3D3D3\", \"marker\": \"D\", \"label\": \"XLNet\"},\n",
    "}\n",
    "\n",
    "def build_legend_handles(style_dict, dashed_note=False):\n",
    "    \"\"\"\n",
    "    Build legend handles for consistent publication figures.\n",
    "    \"\"\"\n",
    "    handles = [\n",
    "        Line2D([0],[0], color=p[\"color\"], marker=p[\"marker\"],\n",
    "               linestyle=\"-\", linewidth=1, markersize=6)\n",
    "        for p in style_dict.values()\n",
    "    ]\n",
    "    labels = [p[\"label\"] for p in style_dict.values()]\n",
    "    if dashed_note:\n",
    "        handles.append(Line2D([0],[0], color=\"black\", linestyle=\"--\", linewidth=1))\n",
    "        labels.append(\"Fine-Tuned Versions\")\n",
    "    return handles, labels"
   ],
   "id": "6d3a2d5b904dd95f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Functions to plot temperature, data size, and data distribution variations",
   "id": "af154657f337fd7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot performance curves over temperature for LLMs\n",
    "def plot_llm_temp(classification_df, metric, ylabel, title, filename, ylim):\n",
    "    \"\"\"\n",
    "    Plot LLM results as a function of temperature.\n",
    "    Used for Figure 6 (main paper).\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    for (model, ft), grp in classification_df.groupby([\"model\", \"fine_tuned\"]):\n",
    "        props = STYLE_LLM[model]\n",
    "        ls    = \"--\" if ft else \"-\"\n",
    "        grp   = grp.sort_values(\"temperature\")\n",
    "        ax.plot(grp[\"temperature\"], grp[metric],\n",
    "                color=props[\"color\"], marker=props[\"marker\"],\n",
    "                linestyle=ls, linewidth=1, markersize=6)\n",
    "    handles, labels = build_legend_handles(STYLE_LLM, dashed_note=True)\n",
    "    ax.legend(handles, labels, loc=\"lower left\",\n",
    "              frameon=True, framealpha=.7, edgecolor=\"black\")\n",
    "    ax.set(xlabel=\"Temperature\", ylabel=ylabel, title=title,\n",
    "           xticks=[0, 0.5, 1, 1.5], ylim=ylim)\n",
    "    ax.grid(True)\n",
    "    fig.savefig(os.path.join(VISUAL_DIR, filename))\n",
    "    plt.show()\n",
    "\n",
    "# Plot F1 vs. data size for both non-LLM and LLM models\n",
    "def plot_f1_vs_size(df_dist, df_base_llm, xticks, dist_label):\n",
    "    \"\"\"\n",
    "    Plot F1-score as a function of training data size for\n",
    "    (a) ML/CNN/PLM and (b) LLMs, for a given distribution.\n",
    "    Used for Plots Figures 7 (main paper) and C1 (Online Appendix)\n",
    "    \"\"\"\n",
    "    # non-LLM slice (tfidf only)\n",
    "    df_nonllm = df_dist[\n",
    "        (df_dist[\"category\"] != \"LLM\") &\n",
    "        ((df_dist[\"vectorizer\"] == \"tfidf\") | df_dist[\"vectorizer\"].isna())\n",
    "    ].copy()\n",
    "    if not df_nonllm.empty:\n",
    "        fig, ax = plt.subplots(figsize=(8,5))\n",
    "        for m, grp in df_nonllm.groupby(\"model\"):\n",
    "            props = STYLE_NON[m]\n",
    "            grp   = grp.sort_values(\"size\")\n",
    "            ax.plot(grp[\"size\"], grp[\"f1_macro\"],\n",
    "                    color=props[\"color\"], marker=props[\"marker\"],\n",
    "                    linestyle=\"-\", linewidth=1)\n",
    "        h, l = build_legend_handles(STYLE_NON)\n",
    "        ax.legend(h, l, loc=\"lower right\", frameon=True, framealpha=.7)\n",
    "        ax.set(xlabel=\"Training-Data Size (N)\", ylabel=\"Macro Avg. F1-Score\",\n",
    "               title=f\"Demonstration Study 1: Macro Avg. F1-Score vs. Size\\n({dist_label} Distribution, non-LLM)\",\n",
    "               xticks=xticks, ylim=(0, 0.7))\n",
    "        ax.grid(True); plt.tight_layout()\n",
    "        fig.savefig(os.path.join(VISUAL_DIR, f\"figure_C1_nonLLM_f1-size-{dist}_updates.png\"))\n",
    "        plt.show()\n",
    "\n",
    "    # LLM slice (base point + dashed fine-tune curve)\n",
    "    df_llm_ft = df_dist[\n",
    "        (df_dist[\"category\"] == \"LLM\") &\n",
    "        (df_dist[\"fine_tuned\"] != 0)\n",
    "    ].copy()\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    x0, xmax = 0, xticks[-1]\n",
    "\n",
    "    ## Plot base LLMs as horizontal lines from x=0 (default) to max\n",
    "    for m, row in df_base_llm.groupby(\"model\"):\n",
    "        props = STYLE_LLM[m]\n",
    "        f1    = row[\"f1_macro\"].iloc[0]\n",
    "        ax.scatter(x0, f1, color=props[\"color\"], marker=props[\"marker\"], s=36, zorder=3)\n",
    "        ax.hlines(f1, x0, xmax, color=props[\"color\"], linewidth=1, zorder=2)\n",
    "\n",
    "    ## Dashed curves for fine-tuned LLMs\n",
    "    for m, grp in df_llm_ft.groupby(\"model\"):\n",
    "        props = STYLE_LLM[m]\n",
    "        grp   = grp.sort_values(\"size\")\n",
    "        ax.plot(grp[\"size\"], grp[\"f1_macro\"],\n",
    "                color=props[\"color\"], marker=props[\"marker\"],\n",
    "                linestyle=\"--\", linewidth=1, markersize=6)\n",
    "    h, l = build_legend_handles(STYLE_LLM, dashed_note=True)\n",
    "    xtick_full = [0] + xticks\n",
    "    ax.legend(h, l, loc=\"lower right\", frameon=True, framealpha=.7)\n",
    "    ax.set(xlabel=\"Training-Data Size (N)\", ylabel=\"Macro Avg. F1-Score\",\n",
    "           title=f\"Demonstration Study 1: Macro Avg. F1-Score vs. Size\\n({dist_label} Distribution, LLM)\",\n",
    "           xticks=xtick_full,\n",
    "           xticklabels=['0\\n(default)'] + list(map(str, xticks)),\n",
    "           ylim=(0.2, 0.8))\n",
    "    ax.grid(True); plt.tight_layout()\n",
    "    fig.savefig(os.path.join(VISUAL_DIR,f\"figure_7_LLM_f1-size-{dist}_updates.png\"))\n",
    "    plt.show()"
   ],
   "id": "34a937e830cb4a5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plot and export all figures",
   "id": "900ce321fd12ab0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# For temperature plots (LLM only)\n",
    "wanted_llm = list(STYLE_LLM.keys())\n",
    "df_plot = classification_df[\n",
    "    (classification_df[\"model\"].isin(wanted_llm)) &\n",
    "    ((classification_df[\"distribution\"] == \"real\") | classification_df[\"distribution\"].isna()) &\n",
    "    ((classification_df[\"size\"] == 2000) | classification_df[\"size\"].isna()) &\n",
    "    (classification_df[\"prompt_type\"] == \"default\")\n",
    "].copy()\n",
    "\n",
    "# For data size plots: combine ML/CNN/PLM with LLMs\n",
    "df_mlcnnplm = classification_df[classification_df[\"category\"] != \"LLM\"]\n",
    "df_llm_sel  = classification_df[(classification_df[\"category\"] == \"LLM\") & (classification_df[\"model\"].isin(wanted_llm))]\n",
    "df_combined = pd.concat([df_mlcnnplm, df_llm_sel], ignore_index=True)\n",
    "\n",
    "# Conditioned on default runs\n",
    "df_cond = df_combined[\n",
    "    ((df_combined[\"prompt_type\"] == \"default\") | df_combined[\"prompt_type\"].isna()) &\n",
    "    ((df_combined[\"temperature\"] == 0.0) | df_combined[\"temperature\"].isna()) &\n",
    "    ((df_combined[\"vectorizer\"] == \"tfidf\") | df_combined[\"vectorizer\"].isna())\n",
    "].copy()"
   ],
   "id": "c778b1a9aa3e8fc5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Temperature curves: Figure 6 (F1, Consistency vs. Temperature)\n",
    "plot_llm_temp(\n",
    "    df_plot,\n",
    "    metric   =\"f1_macro\",\n",
    "    ylabel   =\"Macro Avg. F1-Score\",\n",
    "    title    =\"Demonstration Study 1: Macro Avg. F1-Score and Temperature\",\n",
    "    filename =\"figure_6_LLM_f1-temperature_reviews.png\",\n",
    "    ylim     =(0.25, .80),\n",
    ")\n",
    "plot_llm_temp(\n",
    "    df_plot,\n",
    "    metric   =\"consistency_all_runs\",\n",
    "    ylabel   =\"Consistency Rate\",\n",
    "    title    =\"Demonstration Study 1: Consistency Rate and Temperature\",\n",
    "    filename =\"figure_6_LLM_consistency-temperature_reviews.png\",\n",
    "    ylim     =(0.80, 1.00),\n",
    ")\n",
    "\n",
    "# F1 vs size: Figures 7 and C1 (LLM and non-LLM)\n",
    "label_map = {\"real\": \"Representative\", \"equal\": \"Balanced\"}\n",
    "xticks = [100, 250, 500, 1000, 2000]\n",
    "\n",
    "for dist in [\"real\", \"equal\"]:\n",
    "    dist_lbl = label_map[dist]\n",
    "    df_dist = df_cond[\n",
    "        (df_cond[\"distribution\"] == dist) & df_cond[\"size\"].notna()\n",
    "    ].copy()\n",
    "\n",
    "    df_base_llm = df_cond[\n",
    "        (df_cond[\"category\"] == \"LLM\") &\n",
    "        (df_cond[\"fine_tuned\"] == 0) &\n",
    "        (df_cond[\"model\"].isin(STYLE_LLM.keys()))\n",
    "    ].copy()\n",
    "    plot_f1_vs_size(df_dist, df_base_llm, xticks, dist_lbl)"
   ],
   "id": "d5713b0e2374dec",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
