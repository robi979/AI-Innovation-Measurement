{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "To evaluate large language model (LLM) performance at scale, we leveraged batch inference APIs from all three model providers included in the experiments—OpenAI, Mistral, and Anthropic—across all design decision variations.\n",
    "For each provider, a systematic procedure was implemented to generate, upload, and execute batch requests for both base and fine-tuned models, adhering to the respective provider’s API documentation."
   ],
   "id": "60c38c0f898000c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Imports\n",
    " See `requirements.txt` for full dependency versions"
   ],
   "id": "5bc68bd5147202f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "import glob\n",
    "import re\n",
    "\n",
    "from openai import OpenAI\n",
    "from mistralai import Mistral\n",
    "from anthropic import Anthropic\n",
    "from anthropic.types.message_create_params import MessageCreateParamsNonStreaming\n",
    "from anthropic.types.messages.batch_create_params import Request\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ],
   "id": "e9da44f648cc492d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Global Paths, Directories, and Variables",
   "id": "5ff72c6357395e9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define Demo Study path\n",
    "DEMO_PATH   = os.path.abspath(os.path.join(\"..\"))\n",
    "\n",
    "# Define relevant paths\n",
    "API_KEY_DIR = os.path.expanduser(os.getenv(\"API_KEY_DIR\", 'PATH')) # insert path to .txt with API Key\n",
    "PROMPT_DIR = os.path.join(DEMO_PATH, 'LLM_API', 'prompt_templates')\n",
    "LLM_API = os.path.join(DEMO_PATH,'LLM_API')\n",
    "VAL_PATH = os.path.join(DEMO_PATH, 'training_validation_data', 'demo_app_updates_validation_real_1000.csv')\n",
    "\n",
    "OPENAI_BATCH_DIR = os.path.join(LLM_API, 'OpenAI_batches', 'raw')\n",
    "MISTRAL_BATCH_DIR = os.path.join(LLM_API, 'Mistral_batches', 'raw')\n",
    "ANTHROPIC_BATCH_DIR = os.path.join(LLM_API, 'Anthropic_batches')\n",
    "\n",
    "OPENAI_BATCH_RESULTS_DIR = os.path.join(LLM_API, 'OpenAI_batches', \"results\")\n",
    "MISTRAL_BATCH_RESULTS_DIR = os.path.join(LLM_API, 'Mistral_batches', 'results')\n",
    "\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 94032\n",
    "\n",
    "# Max lines per batch (just redundancy, as batch per model size shouldn't exceed this)\n",
    "MAX_LINES_PER_BATCH = 50000\n",
    "\n",
    "# Load API keys\n",
    "with open(os.path.join(API_KEY_DIR, \"anthropic_api_key_ai-measurement.txt\"), encoding=\"utf-8\") as f: ANTHROPIC_API_KEY = f.read().strip()\n",
    "with open(os.path.join(API_KEY_DIR, \"mistral_api_key_ai-measurement.txt\"), encoding=\"utf-8\") as f: MISTRAL_API_KEY = f.read().strip()\n",
    "with open(os.path.join(API_KEY_DIR, \"openai_api_key_ai-measurement.txt\"), encoding=\"utf-8\") as f: OPENAI_API_KEY = f.read().strip()\n",
    "\n",
    "# Define API-relevant URLs and clients\n",
    "MISTRAL_CLIENT =  Mistral(api_key=MISTRAL_API_KEY)\n",
    "OPENAI_CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n",
    "ANTHROPIC_CLIENT = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "### Models for API-requests\n",
    "# OpenAI GPT models (default set)\n",
    "GPT_MODELS = [\n",
    "    \"gpt-3.5-turbo-0125\",\n",
    "    \"gpt-4o-mini-2024-07-18\",\n",
    "    \"gpt-4o-2024-08-06\",\n",
    "    \"gpt-4.1-2025-04-14\",\n",
    "    \"gpt-4.1-mini-2025-04-14\",\n",
    "    \"gpt-4.1-nano-2025-04-14\",\n",
    "    \"o3-2025-04-16\",\n",
    "    \"o4-mini-2025-04-16\",\n",
    "]\n",
    "\n",
    "# Of those, the “o-series” that don't use temperature:\n",
    "GPT_REASONING_MODELS = [\n",
    "    \"o3-2025-04-16\",\n",
    "    \"o4-mini-2025-04-16\",\n",
    "]\n",
    "\n",
    "# OpenAI GPT models (fine-tuned set -> run 4-2 notebook before)\n",
    "GPT_FT_MODELS_PATH = os.path.join(LLM_API, \"fine-tuned_models\", \"GPT_fine-tuned.txt\")\n",
    "\n",
    "# Mistral models (default set)\n",
    "MISTRAL_MODELS = [\n",
    "    \"mistral-large-2411\",\n",
    "    \"mistral-medium-2505\",\n",
    "    \"mistral-small-2503\",\n",
    "    \"open-mistral-nemo-2407\",\n",
    "    \"ministral-8b-2410\",\n",
    "    \"ministral-3b-2410\",\n",
    "]\n",
    "# Mistral models (fine-tuned set -> run 4-2 notebook before)\n",
    "MISTRAL_FT_MODELS_PATH = os.path.join(LLM_API, \"fine-tuned_models\", \"MISTRAL_fine-tuned.txt\")\n",
    "\n",
    "# Anthropic Claude models (default set)\n",
    "CLAUDE_MODELS = [\n",
    "    \"claude-sonnet-4-20250514\",\n",
    "    \"claude-3-7-sonnet-20250219\",\n",
    "    \"claude-opus-4-20250514\",\n",
    "    \"claude-3-5-haiku-20241022\",\n",
    "    \"claude-3-5-sonnet-20241022\",\n",
    "    \"claude-3-haiku-20240307\",\n",
    "    \"claude-3-opus-20240229\",\n",
    "]   # note reasoning models can easily get very expensive\n",
    "###\n",
    "\n",
    "# Load prompt templates\n",
    "PROMPT_FILES = {\n",
    "    \"default\":         \"updates_prompt_default.txt\",\n",
    "    \"few_shot\":        \"updates_prompt_few-shot.txt\",\n",
    "    \"automatic_cot\":   \"updates_prompt_automatic-cot.txt\",\n",
    "    \"manual_cot\":      \"updates_prompt_manual-cot.txt\",\n",
    "    \"contrastive_cot\": \"updates_prompt_contrastive-cot.txt\",\n",
    "}\n",
    "\n",
    "PROMPTS = {\n",
    "    key: open(os.path.join(PROMPT_DIR, fname), encoding=\"utf-8\").read().strip()\n",
    "    for key, fname in PROMPT_FILES.items()\n",
    "}\n",
    "\n",
    "# Other settings\n",
    "TEMPERATURE_RANGE = [0, 0.5, 1.0, 1.5]\n",
    "REPEATED_RUNS = 3"
   ],
   "id": "f36a47c54a96be45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load validation data\n",
    "df = pd.read_csv(VAL_PATH)"
   ],
   "id": "753b56a08691a1b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## OpenAI Batches\n",
    "We generate JSONL batch files for **default** and **fine-tuned** GPT models, upload them, and create 24 h batch-jobs via the OpenAI API."
   ],
   "id": "67a02679cc755c33"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Batch Creation\n",
    "We build one JSONL batch per GPT model. Run either Default oder Fine-Tuned Models."
   ],
   "id": "4dc3ac0374dae609"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Default Models",
   "id": "1fc01352c7573f9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Process for each model\n",
    "for model in GPT_MODELS:\n",
    "    # Initialize container for model’s entries\n",
    "    entries = []\n",
    "    # Iterate over every prompt template\n",
    "    for prompt_name, prompt_template in PROMPTS.items():\n",
    "        # o-series ignores temperature; others get sweep on default prompt\n",
    "        if model in GPT_REASONING_MODELS:\n",
    "            temps = [None]\n",
    "        else:\n",
    "            temps = TEMPERATURE_RANGE if prompt_name == 'default' else [None]\n",
    "\n",
    "        for temp in temps:\n",
    "            for run in range(1, REPEATED_RUNS + 1):\n",
    "                for _, row in df.iterrows():\n",
    "                    # Compose user prompt\n",
    "                    update_text = (row.get('whats_new', '') or '').strip()\n",
    "                    prompt = f\"{prompt_template}\\n\\nApp update text: {update_text}\"\n",
    "                    # Unique ID encodes model/prompt/temp/run/row.id\n",
    "                    custom_id = (\n",
    "                        f\"{model}__{prompt_name}\"\n",
    "                        f\"__t{int((temp or 0) * 10)}__run{run}__id{row['id']}\"\n",
    "                    )\n",
    "                    body = {\n",
    "                        'model': model,\n",
    "                        'messages': [{'role': 'user', 'content': prompt}],\n",
    "                        'max_completion_tokens': 1000,\n",
    "                        'seed': SEED\n",
    "                    }\n",
    "                    if temp is not None:\n",
    "                        body['temperature'] = temp\n",
    "                    if model in GPT_REASONING_MODELS:\n",
    "                        body['reasoning_effort'] = 'low'\n",
    "\n",
    "                    entries.append({\n",
    "                        'custom_id': custom_id,\n",
    "                        'method': 'POST',\n",
    "                        'url': '/v1/chat/completions',\n",
    "                        'body': body\n",
    "                    })\n",
    "\n",
    "    # Paginate entries to ≤ 50,000 lines per file\n",
    "    total = len(entries)\n",
    "    num_files = math.ceil(total / MAX_LINES_PER_BATCH)\n",
    "\n",
    "    for part in range(num_files):\n",
    "        start = part * MAX_LINES_PER_BATCH\n",
    "        end = min(start + MAX_LINES_PER_BATCH, total)\n",
    "        batch_lines = entries[start:end]\n",
    "\n",
    "        # Sanitize model name for filenames\n",
    "        model_safe = model.replace('/', '-')\n",
    "        suffix = f\"{part + 1:02d}\" if num_files > 1 else \"01\"\n",
    "        batch_fname = f\"openai_batch_{model_safe}_{suffix}.jsonl\"\n",
    "        batch_path = os.path.join(OPENAI_BATCH_DIR, batch_fname)\n",
    "\n",
    "        with open(batch_path, 'w', encoding='utf-8') as fout:\n",
    "            for entry in batch_lines:\n",
    "                fout.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "\n",
    "        print(f\"Wrote {len(batch_lines)} entries for {model} to {batch_path} (lines {start + 1}–{end})\")"
   ],
   "id": "c9640a9cd558565b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Fine-Tuned Models\n",
    "Note: Before running this code, run notebook 4-2 to generate fine-tuned models via fine-tuning API."
   ],
   "id": "2504846431543400"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read every non-empty line as a model identifier\n",
    "with open(GPT_FT_MODELS_PATH, encoding=\"utf-8\") as f:\n",
    "    FT_MODELS = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Process for each fine-tuned model\n",
    "for ft_model in FT_MODELS:\n",
    "    # Parse out the \"type–size\" token (e.g. \"3-5-turbo-equal-2000\")\n",
    "    parts = ft_model.split(\":\")\n",
    "    type_size = parts[3] if len(parts) > 3 else \"\"\n",
    "    is_var_model = \"2000\" in type_size\n",
    "\n",
    "    # Initialize container for model's entries\n",
    "    entries = []\n",
    "\n",
    "    if is_var_model:\n",
    "        # Iterate over every prompt template\n",
    "        for prompt_name, prompt_template in PROMPTS.items():\n",
    "            # Only the \"default\" prompt gets a temperature sweep\n",
    "            temps = TEMPERATURE_RANGE if prompt_name == \"default\" else [None]\n",
    "            for temp in temps:\n",
    "                for run in range(1, REPEATED_RUNS + 1):\n",
    "                    for _, row in df.iterrows():\n",
    "                        # Compose user prompt\n",
    "                        update_text = (row.get(\"whats_new\", \"\") or \"\").strip()\n",
    "                        prompt = f\"{prompt_template}\\n\\nApp update text: {update_text}\"\n",
    "                        # Unique ID encodes model/prompt/temp/run/row.id\n",
    "                        custom_id = (\n",
    "                            f\"{ft_model.replace('/', '-')}\"\n",
    "                            f\"__{prompt_name}\"\n",
    "                            f\"__t{int((temp or 0) * 10)}\"\n",
    "                            f\"__run{run}\"\n",
    "                            f\"__id{row['id']}\"\n",
    "                        )\n",
    "                        body = {\n",
    "                            \"model\": ft_model,\n",
    "                            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                            \"max_completion_tokens\": 1000,\n",
    "                            \"seed\": SEED,\n",
    "                        }\n",
    "                        if temp is not None:\n",
    "                            body[\"temperature\"] = temp\n",
    "                        entries.append({\n",
    "                            \"custom_id\": custom_id,\n",
    "                            \"method\": \"POST\",\n",
    "                            \"url\": \"/v1/chat/completions\",\n",
    "                            \"body\": body\n",
    "                        })\n",
    "    else:\n",
    "        # Only 3 runs, default prompt, no temp variation\n",
    "        prompt_template = PROMPTS[\"default\"]\n",
    "        for run in range(1, REPEATED_RUNS + 1):\n",
    "            for _, row in df.iterrows():\n",
    "                # Compose user prompt\n",
    "                update_text = (row.get(\"whats_new\", \"\") or \"\").strip()\n",
    "                prompt = f\"{prompt_template}\\n\\nApp update text: {update_text}\"\n",
    "                # Unique ID encodes model/prompt/temp/run/row.id\n",
    "                custom_id = (\n",
    "                    f\"{ft_model.replace('/', '-')}\"\n",
    "                    f\"__default\"\n",
    "                    f\"__t0\"\n",
    "                    f\"__run{run}\"\n",
    "                    f\"__id{row['id']}\"\n",
    "                )\n",
    "                body = {\n",
    "                    \"model\": ft_model,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    \"max_completion_tokens\": 1000,\n",
    "                    \"seed\": SEED,\n",
    "                }\n",
    "                entries.append({\n",
    "                    \"custom_id\": custom_id,\n",
    "                    \"method\": \"POST\",\n",
    "                    \"url\": \"/v1/chat/completions\",\n",
    "                    \"body\": body\n",
    "                })\n",
    "\n",
    "    # Paginate entries to ≤ 50,000 lines per file\n",
    "    total = len(entries)\n",
    "    num_files = math.ceil(total / MAX_LINES_PER_BATCH)\n",
    "    # Sanitize model name for filenames\n",
    "    model_safe = re.sub(r'[:/\\\\\\s]+', '-', ft_model)\n",
    "    for part in range(num_files):\n",
    "        start = part * MAX_LINES_PER_BATCH\n",
    "        end = min(start + MAX_LINES_PER_BATCH, total)\n",
    "        batch_lines = entries[start:end]\n",
    "\n",
    "        suffix = f\"{part + 1:02d}\" if num_files > 1 else \"01\"\n",
    "        batch_fname = f\"openai_batch_{model_safe}_{suffix}.jsonl\"\n",
    "        batch_path = os.path.join(OPENAI_BATCH_DIR, batch_fname)\n",
    "\n",
    "        with open(batch_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "            for entry in batch_lines:\n",
    "                fout.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        print(f\"Wrote {len(batch_lines)} entries for {ft_model} to {batch_path}\"\n",
    "              f\" (lines {start + 1}–{end})\")"
   ],
   "id": "29eba8e906dd0bb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Upload Batches\n",
    "We upload every JSONL batch to OpenAI and capture file IDs. Run either batch creation for default models or fine-tuned models before."
   ],
   "id": "2509eda64c727370"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Path to batch files\n",
    "batch_paths = sorted(glob.glob(os.path.join(OPENAI_BATCH_DIR, \"openai_batch_*.jsonl\")))\n",
    "\n",
    "# Upload each batch file and collect its file ID\n",
    "uploaded_file_ids = []\n",
    "for path in batch_paths:\n",
    "    print(f\"Uploading {path}...\")\n",
    "    with open(path, 'rb') as f:\n",
    "        file_resp = OPENAI_CLIENT.files.create(\n",
    "            file=f,\n",
    "            purpose='batch'\n",
    "        )\n",
    "    print(f\"Uploaded: id={file_resp.id}, filename={file_resp.filename}, bytes={file_resp.bytes}\")\n",
    "    uploaded_file_ids.append(file_resp.id)\n",
    "\n",
    "print(\"All batch files uploaded.\\n\")"
   ],
   "id": "cb5a348208f56347",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create Batch Jobs\n",
    "We create a 24 h chat-completion batch job for each uploaded file"
   ],
   "id": "ecbcf57f7ee3abd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a Batch for each uploaded file\n",
    "for file_id in uploaded_file_ids:\n",
    "    batch = OPENAI_CLIENT.batches.create(\n",
    "        input_file_id=file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "            \"description\": f\"eval job for {file_id}\"\n",
    "        }\n",
    "    )\n",
    "    print(f\"Created batch {batch.id} for {file_id}, status: {batch.status}\")"
   ],
   "id": "e38e61616ea89440",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Mistral Batches\n",
    "We generate JSONL batch files for default and fine-tuned Mistral models, upload them, and create batch-jobs via the Mistral API."
   ],
   "id": "5d9783de1d600d6e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Batch Creation\n",
    "We build one JSONL batch per Mistral model. Run either Default or Fine-Tuned Models."
   ],
   "id": "d0852972cb497609"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Default Models",
   "id": "fc33628abd544a77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Process for each model\n",
    "for model in MISTRAL_MODELS:\n",
    "    # Initialize container for model's entries\n",
    "    entries = []\n",
    "    for prompt_name, prompt_template in PROMPTS.items():\n",
    "        # Only the \"default\" prompt gets a temperature sweep\n",
    "        temps = TEMPERATURE_RANGE if prompt_name == 'default' else [None]\n",
    "        for temp in temps:\n",
    "            for run in range(1, REPEATED_RUNS + 1):\n",
    "                for _, row in df.iterrows():\n",
    "                    # Compose user prompt\n",
    "                    update_text = (row.get('whats_new', '') or '').strip()\n",
    "                    prompt = f\"{prompt_template}\\n\\nApp update text: {update_text}\"\n",
    "                    # Unique ID encodes model/prompt/temp/run/row.id\n",
    "                    custom_id = (\n",
    "                        f\"{model}__{prompt_name}\"\n",
    "                        f\"__t{int((temp or 0) * 10)}__run{run}__id{row['id']}\"\n",
    "                    )\n",
    "                    body = {\n",
    "                        \"max_tokens\": 1000,\n",
    "                        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                        **({\"temperature\": temp} if temp is not None else {}),\n",
    "                        \"random_seed\": SEED,\n",
    "                    }\n",
    "                    entries.append({\n",
    "                        \"custom_id\": custom_id,\n",
    "                        \"body\": body\n",
    "                    })\n",
    "\n",
    "   # Paginate entries to ≤ 50,000 lines per file\n",
    "    total = len(entries)\n",
    "    num_files = math.ceil(total / MAX_LINES_PER_BATCH)\n",
    "\n",
    "    for part in range(num_files):\n",
    "        start = part * MAX_LINES_PER_BATCH\n",
    "        end = min(start + MAX_LINES_PER_BATCH, total)\n",
    "        batch_lines = entries[start:end]\n",
    "\n",
    "        # Sanitize model name for filenames\n",
    "        suffix = f\"{part + 1:02d}\" if num_files > 1 else \"01\"\n",
    "        fname = f\"mistral_batch_{model.replace('/', '-')}_{suffix}.jsonl\"\n",
    "        path = os.path.join(MISTRAL_BATCH_DIR, fname)\n",
    "\n",
    "        with open(path, 'w', encoding='utf-8') as fout:\n",
    "            for entry in batch_lines:\n",
    "                fout.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "        print(f\"Wrote {len(batch_lines)} entries for {model} to {path}\")"
   ],
   "id": "ca5970ed467acea4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Fine-Tuned Models\n",
    "Note: Before running this code, execute notebook 4-2 to generate fine-tuned models via fine-tuning API."
   ],
   "id": "639352aecbae1efc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read every non-empty line as a model identifier\n",
    "with open(MISTRAL_FT_MODELS_PATH, encoding=\"utf-8\") as f:\n",
    "    raw_ft = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Process for each fine-tuned model\n",
    "for ft_full in raw_ft:\n",
    "    # Parse out the model components\n",
    "    parts = ft_full.split(\":\")\n",
    "    if len(parts) < 5:\n",
    "        raise ValueError(f\"Bad line in MISTRAL_fine-tuned.txt: {ft_full}\")\n",
    "    model_id = \":\".join(parts[:4])\n",
    "    suffix   = parts[4]\n",
    "    # Sanitize model name for filenames\n",
    "    safe_ft  = re.sub(r\"[:/\\\\\\s]+\", \"-\", ft_full)\n",
    "\n",
    "    # Initialize container for model's entries\n",
    "    entries = []\n",
    "    is_var  = \"real_2000\" in suffix\n",
    "\n",
    "    if is_var:\n",
    "        # Iterate over every prompt template\n",
    "        for prompt_name, prompt_template in PROMPTS.items():\n",
    "            # Only the \"default\" prompt gets a temperature sweep\n",
    "            temps = TEMPERATURE_RANGE if prompt_name == \"default\" else [None]\n",
    "            for temp in temps:\n",
    "                for run in range(1, REPEATED_RUNS + 1):\n",
    "                    for _, row in df.iterrows():\n",
    "                        # Compose user prompt\n",
    "                        txt = (row.whats_new or \"\").strip()\n",
    "                        prompt = f\"{prompt_template}\\n\\nApp update text: {txt}\"\n",
    "                        # Unique ID encodes model/prompt/temp/run/row.id\n",
    "                        cid = (\n",
    "                            f\"{safe_ft}\"\n",
    "                            f\"__{prompt_name}\"\n",
    "                            f\"__t{int((temp or 0)*10)}\"\n",
    "                            f\"__run{run}\"\n",
    "                            f\"__id{row.id}\"\n",
    "                        )\n",
    "                        body = {\n",
    "                            \"max_tokens\":   1000,\n",
    "                            \"messages\":    [{\"role\":\"user\",\"content\":prompt}],\n",
    "                            \"random_seed\": SEED,\n",
    "                            **({\"temperature\": temp} if temp is not None else {})\n",
    "                        }\n",
    "                        entries.append({\"custom_id\": cid, \"body\": body})\n",
    "\n",
    "    else:\n",
    "        # Only 3 runs, default prompt, no temp variation\n",
    "        template = PROMPTS[\"default\"]\n",
    "        for run in range(1, REPEATED_RUNS + 1):\n",
    "            for _, row in df.iterrows():\n",
    "                # Compose user prompt\n",
    "                txt = (row.whats_new or \"\").strip()\n",
    "                prompt = f\"{template}\\n\\nApp update text: {txt}\"\n",
    "                # Unique ID encodes model/prompt/temp/run/row.id\n",
    "                cid = (\n",
    "                    f\"{safe_ft}\"\n",
    "                    \"__default\"\n",
    "                    f\"__t0\"\n",
    "                    f\"__run{run}\"\n",
    "                    f\"__id{row.id}\"\n",
    "                )\n",
    "                body = {\n",
    "                    \"max_tokens\":   1000,\n",
    "                    \"messages\":    [{\"role\":\"user\",\"content\":prompt}],\n",
    "                    \"random_seed\": SEED\n",
    "                }\n",
    "                entries.append({\"custom_id\": cid, \"body\": body})\n",
    "\n",
    "    # Paginate entries to ≤ 50,000 lines per file\n",
    "    total     = len(entries)\n",
    "    num_files = math.ceil(total / MAX_LINES_PER_BATCH)\n",
    "    for part in range(num_files):\n",
    "        chunk = entries[part*MAX_LINES_PER_BATCH:(part+1)*MAX_LINES_PER_BATCH]\n",
    "\n",
    "        # Sanitize model name for filenames\n",
    "        idx   = f\"{part+1:02d}\" if num_files > 1 else \"01\"\n",
    "        fname = f\"mistral_batch_{safe_ft}_{idx}.jsonl\"\n",
    "        path  = os.path.join(MISTRAL_BATCH_DIR, fname)\n",
    "\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as fout:\n",
    "            for e in chunk:\n",
    "                fout.write(json.dumps(e, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        print(f\"Wrote {len(chunk)} entries for {ft_full} → {path}\")"
   ],
   "id": "e24d33b972a7c2e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Upload Batches\n",
    "We upload every JSONL batch to Mistral and capture file IDs. Run either batch creation for default models or fine-tuned models before."
   ],
   "id": "ea9157db8cd0dfca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Path to batch files\n",
    "batch_paths = sorted(glob.glob(os.path.join(MISTRAL_BATCH_DIR, \"mistral_batch_*.jsonl\")))\n",
    "\n",
    "# Upload each batch file and collect its file ID\n",
    "uploaded = []\n",
    "for path in batch_paths:\n",
    "    fname = os.path.basename(path)\n",
    "    model = fname.split(\"_\")[2]\n",
    "\n",
    "    print(f\"Uploading {fname} (model={model})…\")\n",
    "    with open(path, 'rb') as f:\n",
    "        up = MISTRAL_CLIENT.files.upload(\n",
    "            file={\"file_name\": fname, \"content\": f},\n",
    "            purpose=\"batch\"\n",
    "        )\n",
    "    print(f\"Uploaded: id={up.id}\")\n",
    "    uploaded.append({\"model\": model, \"file_id\": up.id})\n",
    "\n",
    "print(\"All Mistral batch files uploaded.\\n\")"
   ],
   "id": "e3a1aec05ffaddbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create Batch Jobs\n",
    "We create a batch job for each uploaded file."
   ],
   "id": "98b0e832e5bc9c8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a batch job for each uploaded file\n",
    "for item in uploaded:\n",
    "    file_id = item[\"file_id\"]\n",
    "    model = item[\"model\"]\n",
    "    # Parse model name for fine-tuned models\n",
    "    if model.startswith(\"ft-\"):\n",
    "        parts = model.split(\"-\")\n",
    "        # parts = [\"ft\",\"mistral\",\"small\",\"latest\",\"d1ef7e20\",\"20250527\",\"314de464\",\"equal_500\"]\n",
    "\n",
    "        prefix = parts[0]\n",
    "        model_name = \"-\".join(parts[1:4])   # \"mistral-small-latest\"\n",
    "        rev = parts[4]              # \"d1ef7e20\"\n",
    "        date = parts[5]              # \"20250527\"\n",
    "        sha = parts[6]              # \"314de464\"\n",
    "\n",
    "        real_model = f\"{prefix}:{model_name}:{rev}:{date}:{sha}\"\n",
    "        # \"ft:mistral-small-latest:d1ef7e20:20250527:314de464\"\n",
    "    else:\n",
    "        real_model = model\n",
    "\n",
    "    # Submit batch job to Mistral API\n",
    "    job = MISTRAL_CLIENT.batch.jobs.create(\n",
    "        input_files=[file_id],\n",
    "        model=real_model,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        metadata={\"description\": f\"classification batch for {real_model}\"}\n",
    "    )\n",
    "    print(f\"Created batch job {job.id} for file {file_id} (model={real_model}), status={job.status}\")"
   ],
   "id": "619ab0bbe99910b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Anthropic Batches\n",
    "We create batch requests for Claude models and process JSONL results into CSV format."
   ],
   "id": "c434387bb42d2e80"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Batch Requests\n",
    "We build batch requests for each Claude model with various prompts and temperature settings."
   ],
   "id": "6aec7027f705b771"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Shorten model names as custom_id max is 64 characters\n",
    "def short_model_name(full_model: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts full Claude model names to shortened versions for custom_id usage.\n",
    "    # e.g. \"claude-3-7-sonnet-20250219\" → [\"claude\",\"3\",\"7\",\"sonnet\",\"20250219\"]\n",
    "    \"\"\"\n",
    "    parts = full_model.split(\"-\")\n",
    "    # Drop the first (\"claude\") and last (date) parts, then re-join\n",
    "    return \"-\".join(parts[1:-1])"
   ],
   "id": "72513c6c6f273617",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Process for each model\n",
    "for model in CLAUDE_MODELS:\n",
    "    short_model = short_model_name(model)\n",
    "    batch_requests = []\n",
    "\n",
    "    # Iterate through all prompt templates\n",
    "    for prompt_name, prompt_template in PROMPTS.items():\n",
    "        # Only the \"default\" prompt gets a temperature sweep\n",
    "        base_temps = TEMPERATURE_RANGE if prompt_name == \"default\" else [None]\n",
    "        temps = [t for t in base_temps if t is None or t <= 1.0]\n",
    "\n",
    "        for temp in temps:\n",
    "            for run in range(1, REPEATED_RUNS + 1):\n",
    "                for _, row in df.iterrows():\n",
    "                    # Compose user prompt\n",
    "                    update_text = (row.get(\"whats_new\", \"\") or \"\").strip()\n",
    "                    prompt = f\"{prompt_template}\\n\\nApp update text: {update_text}\"\n",
    "\n",
    "                    # Unique ID encodes model/prompt/temp/run/row.id\n",
    "                    raw_id = (\n",
    "                        f\"{short_model}_{prompt_name}\"\n",
    "                        f\"_t{int((temp or 0) * 10)}_r{run}_i{row['id']}\"\n",
    "                    )\n",
    "                    # Ensure ≤64 chars for API limits (redundancy)\n",
    "                    custom_id = (\n",
    "                        raw_id[:64]\n",
    "                        if len(raw_id) <= 64\n",
    "                        else raw_id[:50]  # or hash fallback\n",
    "                    )\n",
    "\n",
    "                    # Build request parameters\n",
    "                    params = MessageCreateParamsNonStreaming(\n",
    "                        model=model,\n",
    "                        max_tokens=1000,\n",
    "                        temperature=(temp or 0),\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    )\n",
    "                    batch_requests.append(Request(custom_id=custom_id, params=params))\n",
    "\n",
    "    # Submit the complete batch for model\n",
    "    message_batch = ANTHROPIC_CLIENT.messages.batches.create(\n",
    "        requests=batch_requests\n",
    "    )\n",
    "    print(\n",
    "        f\"Created batch for {model} \"\n",
    "        f\"with {len(batch_requests)} requests: \"\n",
    "        f\"{message_batch.id} (status={message_batch.processing_status})\"\n",
    "    )"
   ],
   "id": "d7f0f8b8b516af0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Process results\n",
    "We process JSONL result files and convert them to CSV format for analysis. Note: Download batch files from each provider's developer platform before."
   ],
   "id": "9057656827634055"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load validation dataset and prepare ID columns for matching with batch results\n",
    "df = pd.read_csv(VAL_PATH, dtype={'id': str})\n",
    "df['id'] = df['id'].str.strip()"
   ],
   "id": "80455e4a9ccf4fc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Helper Functions\n",
    "Utility functions for ID normalization and record parsing across different LLM providers."
   ],
   "id": "a5644509edfd497b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Helper: normalize ID string by removing 'id' prefix",
   "id": "348dcf96529d5ca8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def normalize_id_str(id_part):\n",
    "    \"\"\"Clean ID strings by removing 'id' prefix and whitespace\"\"\"\n",
    "    return id_part.replace('id', '').strip()"
   ],
   "id": "3cb06f4c42924d5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Provider-Specific Parsers\n",
    "Each LLM provider has different JSONL response formats requiring specialized parsing."
   ],
   "id": "9d73562abb7460f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def parse_openai_record(record):\n",
    "    \"\"\"Parse OpenAI batch response record\"\"\"\n",
    "    # Split custom_id into components\n",
    "    parts = record.get('custom_id', '').split('__')\n",
    "    if len(parts) < 5:\n",
    "        return None\n",
    "    model, prompt_type, temp, run, id_part = parts[:5]\n",
    "    id_str = normalize_id_str(id_part)\n",
    "\n",
    "    # Create standardized column name\n",
    "    col = f\"{model}__{prompt_type}__{temp}__{run}\".replace(':','_').replace('-','_').replace('.','_')\n",
    "\n",
    "    # Extract response content\n",
    "    try:\n",
    "        content = record['response']['body']['choices'][0]['message']['content']\n",
    "    except Exception:\n",
    "        return None\n",
    "    return id_str, col, content\n",
    "\n",
    "def parse_mistral_record(record):\n",
    "    \"\"\"Parse Mistral batch response record\"\"\"\n",
    "    # Split custom_id into components\n",
    "    parts = record.get('custom_id', '').split('__')\n",
    "    if len(parts) < 5:\n",
    "        return None\n",
    "    model, prompt_type, temp, run, id_part = parts[:5]\n",
    "    id_str = normalize_id_str(id_part)\n",
    "\n",
    "    # Create standardized column name\n",
    "    col = f\"{model}__{prompt_type}__{temp}__{run}\".replace(':','_').replace('-','_').replace('.','_')\n",
    "\n",
    "    # Extract response content\n",
    "    try:\n",
    "        content = record['response']['body']['choices'][0]['message']['content']\n",
    "    except Exception:\n",
    "        return None\n",
    "    return id_str, col, content\n",
    "\n",
    "def parse_anthropic_record(record):\n",
    "    \"\"\"Parse Anthropic batch response record\"\"\"\n",
    "    # Split custom_id into components (single underscore separator)\n",
    "    parts = record.get('custom_id', '').split('_')\n",
    "    if len(parts) < 5:\n",
    "        return None\n",
    "\n",
    "    # Extract components from different positions due to Anthropic format\n",
    "    id_part = parts[-1]\n",
    "    run = parts[-2]\n",
    "    temp = parts[-3]\n",
    "    model = parts[0]\n",
    "    prompt_type = '_'.join(parts[1:-3])\n",
    "    id_str = normalize_id_str(id_part.lstrip('i'))\n",
    "\n",
    "    # Create standardized column name\n",
    "    col = f\"{model}__{prompt_type}__{temp}__{run}\".replace(':','_').replace('-','_').replace('.','_')\n",
    "\n",
    "    # Extract response content\n",
    "    msg = record.get('result', {}).get('message', {})\n",
    "    content = ''\n",
    "    try:\n",
    "        content = msg['content'][0]['text']\n",
    "    except Exception:\n",
    "        # If content missing or malformed, leave empty\n",
    "        pass\n",
    "    return id_str, col, content"
   ],
   "id": "623486580d4244e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### File Processing Functions\n",
    "Core functions for processing JSONL files and parallel execution."
   ],
   "id": "8b8c4b49d9fc1653"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_file(path, parser):\n",
    "    \"\"\"Process a single JSONL file using the specified parser\"\"\"\n",
    "    mapping = {}\n",
    "    print(f\"Processing {path.name}\")\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip malformed JSON lines\n",
    "                continue\n",
    "            # Parse record using provider-specific parser\n",
    "            parsed = parser(rec)\n",
    "            if parsed:\n",
    "                id_str, col, content = parsed\n",
    "                # Group by column, then by ID\n",
    "                mapping.setdefault(col, {})[id_str] = content\n",
    "\n",
    "    print(f\"  Mapped {len(mapping)} columns from {path.name}\")\n",
    "    return mapping\n",
    "\n",
    "def process_results_parallel(directory, parser, df, workers=10):\n",
    "    \"\"\"Process all JSONL files in a directory using parallel execution.\"\"\"\n",
    "    # Find all JSONL files in directory and subdirectories\n",
    "    files = list(Path(directory).rglob('*.jsonl'))\n",
    "    print(f\"Found {len(files)} files in {directory}\")\n",
    "\n",
    "    combined = {}\n",
    "\n",
    "    # Process files in parallel\n",
    "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        # Submit all file processing tasks\n",
    "        futures = {executor.submit(process_file, f, parser): f for f in files}\n",
    "\n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(futures):\n",
    "            file_map = future.result()\n",
    "            # Merge results from each file\n",
    "            for col, id_map in file_map.items():\n",
    "                combined.setdefault(col, {}).update(id_map)\n",
    "\n",
    "    # Add new columns to dataframe\n",
    "    for col, id_map in combined.items():\n",
    "        df[col] = df['id'].map(id_map)\n",
    "\n",
    "    return df.copy()"
   ],
   "id": "281215772d4ea7c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Process All Provider Results\n",
    "Execute processing for each LLM provider in sequence."
   ],
   "id": "3a794e90c24952d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = process_results_parallel(OPENAI_BATCH_RESULTS_DIR, parse_openai_record, df)\n",
    "df = process_results_parallel(MISTRAL_BATCH_RESULTS_DIR, parse_mistral_record, df)\n",
    "df = process_results_parallel(ANTHROPIC_BATCH_DIR, parse_anthropic_record, df)"
   ],
   "id": "b4d05371150991d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save final output\n",
    "df.to_csv(os.path.join(DEMO_PATH, 'output_data', 'validation_with_model_preds_LLM.csv'), index=False)"
   ],
   "id": "b02481924e211f74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Clean Results\n",
    "Standardize raw model outputs into valid single-label strings (1–7).\n",
    "Cleans and validates outputs from LLMs or API responses, ensuring only acceptable class labels are retained."
   ],
   "id": "e80cc1ad96b749f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cleaning Prep and Helper Function",
   "id": "5c450cfaf10ff0e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Columns to exclude from label cleaning\n",
    "EXCLUDE_COLS = [\n",
    "    'release_date', 'version_display', 'whats_new', 'body', 'update_classification', 'id', 'app_id', 'previous_version', 'previous_release_date', 'previous_id'\n",
    "]\n",
    "\n",
    "def extract_single_label(text):\n",
    "    \"\"\"\n",
    "    Normalize prediction output into a single class label string (1–7).\n",
    "    Returns '0' as a fallback for invalid predictions.\n",
    "\n",
    "    Handles:\n",
    "    - Integers or floats like 3, 3.0\n",
    "    - Strings like '3', ' (3) ', or even '3 ; 4' (keeps only last valid single label)\n",
    "    \"\"\"\n",
    "    if isinstance(text, float):\n",
    "        if pd.isna(text):\n",
    "            return '0'  # fallback\n",
    "        if text.is_integer() and 1 <= int(text) <= 7:\n",
    "            return str(int(text))\n",
    "        return '0'\n",
    "\n",
    "    if isinstance(text, int):\n",
    "        return str(text) if 1 <= text <= 7 else '0'\n",
    "\n",
    "    # Parse strings and try to extract digits in range 1–7\n",
    "    text = str(text)\n",
    "    pattern = r'\\(?\\s*([1-7](?:\\s*;\\s*[1-7])*)\\s*\\)?'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return '0'\n",
    "    last = matches[-1]\n",
    "    cleaned = [d for d in re.split(r'\\s*;\\s*', last.strip('; ')) if d.isdigit() and 1 <= int(d) <= 7]\n",
    "    return cleaned[-1] if cleaned else '0'"
   ],
   "id": "47980869480f866c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Model Output File and Apply Cleaning",
   "id": "25631454efcbed60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv(os.path.join(DEMO_PATH, 'output_data', 'validation_with_model_preds_LLM.csv'))"
   ],
   "id": "bc8d4ba93c4e1215",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Identify prediction columns (exclude inputs/metadata)\n",
    "model_cols = [c for c in df.columns if c not in EXCLUDE_COLS]\n",
    "\n",
    "# Apply label cleaning function to each model column\n",
    "for col in model_cols:\n",
    "    df[col] = df[col].apply(extract_single_label)\n",
    "\n",
    "# Save cleaned predictions to file\n",
    "save_path = os.path.join(DEMO_PATH, 'output_data', 'validation_with_model_preds_LLM_cleaned.csv')\n",
    "df.to_csv(save_path, index=False)"
   ],
   "id": "419fe81344ffd1f2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
