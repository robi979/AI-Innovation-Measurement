{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "From a large dataset covering the complete update history of every app active on the Apple App Store between 2018 and 2021, we randomly sampled 4,000 update descriptions to form a pool for creating training and validation datasets. To ensure consistent analysis, we filtered the update descriptions to include only those written in English.  Each update description contains a corresponding release note and version number. On the App Store, these release notes are displayed in a section titled “What’s New” and limited to 4,000 characters. Since 2018, it is mandatory for developers to specify update details and avoid generic texts.\n",
    "To establish ground truth for the sample pool, a member of the research team and a research assistant coded the release notes in the pool according to a coding scheme developed by the research team through inductive coding. This scheme comprises seven content dimensions (classes) and was validated   in interviews with software application developers (detailed in Section 4.1.2). A subset of n=1,000 update descriptions was independently coded by both raters to ensure high inter-rater reliability (observed agreement = 0.91, κ = 0.87). Any discrepancies were discussed to be consistent across the remaining sample pool."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddce85512b6bcd9c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Imports\n",
    " See `requirements.txt` for full dependency versions"
   ],
   "id": "f0d7589d6841bca6"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be751a37a3b78ccf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Global Paths, Directories and Variables",
   "id": "dc59a2aa6c0a6143"
  },
  {
   "cell_type": "code",
   "source": [
    "# Define relevant paths\n",
    "DEMO_PATH   = os.path.abspath(os.path.join(\"..\"))\n",
    "\n",
    "# Define relevant paths\n",
    "DATA_DIR = os.path.join(DEMO_PATH, 'training_validation_data')\n",
    "LLM_API_FOLDER = os.path.join(DEMO_PATH, 'LLM_API')\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 94032"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f54945ee2124ef36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Helper function for class distribution\n",
    "def print_dist(df_obj, name, label_col='update_classification'):\n",
    "    \"\"\"\n",
    "    Print the count and percentage distribution of 'update_classification' classes for the given DataFrame.\n",
    "    \"\"\"\n",
    "    dist = df_obj[label_col].value_counts().sort_index()\n",
    "    pct  = (dist / len(df_obj) * 100).round(2)\n",
    "    dist_df = pd.DataFrame({'count': dist, 'pct': pct})\n",
    "    print(f\"{name} distribution (n={len(df_obj)}):\")\n",
    "    print(dist_df)"
   ],
   "id": "e6fb3f390caf938a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Pool\n",
    "\n",
    "We load the manually labeled dataset from the CSV file located in `DATA_DIR`. The dataset includes release notes (`whats_new`), their IDs, dates, and update classifications.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc2394a4fee2f37b"
  },
  {
   "cell_type": "code",
   "source": [
    "# Load manually labeled data pool for training and validation splits for model fine-tuning and performance evaluation \n",
    "df = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, \"demo_app_updates_full_coded_4000.csv\"),\n",
    "    header=0,\n",
    "    dtype={'id': str, 'whats_new': str, 'update_classification': int},\n",
    "    parse_dates=['release_date'],\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "# Summary of original class distribution\n",
    "print_dist(df, \"Data Pool\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ee1bdc72480caf4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validation and Training Data Splits\n",
    "We generate random subsamples from the labeled sample pool pool of n=4,000 release notes: a validation set of n = 1,000 release notes serving as a holdout and training sets with varying sample sizes of n = 2,000; 1,000; 500; 250; 100 release notes. Subsamples were constructed to reflect both a class distribution similar to that of the overall pool (representative) and, when possible, an equally balanced class distribution.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce6873c3b5804725"
  },
  {
   "cell_type": "code",
   "source": [
    "# Define constants (here set based on data pool class distribution)\n",
    "VAL_SIZE        = 1000  # Number of validation samples to hold out\n",
    "MIN_PER_CLASS   = 3     # Minimum labeled texts per class when sampling training data\n",
    "TRAINING_SIZES  = [2000, 1000, 500, 250, 100]  # Various training set sizes to generate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2e3ef2a1436ee7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Validation Set Creation\n",
    "\n",
    "We create a validation set of size `VAL_SIZE`. The remaining data forms the training pool."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdd8cdc809944b51"
  },
  {
   "cell_type": "code",
   "source": [
    "# Create validation and training-remaining pools via stratified split, ensures each class is represented proportionally in validation\n",
    "df_train_remaining, df_val_final = train_test_split(\n",
    "    df,\n",
    "    test_size=VAL_SIZE,\n",
    "    stratify=df['update_classification'],\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "print_dist(df_val_final, \"Validation set\")\n",
    "print_dist(df_train_remaining, \"Training-remaining pool\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fe6d45ca607cc25",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Set Creation\n",
    "\n",
    "We generate training sets of sizes specified in `TRAINING_SIZES`, both real weighted and equal-distribution.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d049af6d08f483e"
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": [
    "## Sampling Functions\n",
    "\n",
    "We define helper functions to sample training subsets either reflecting representative (real-world) distributions or a balanced (equal) class distribution, with at least `MIN_PER_CLASS` samples per class.\n"
   ],
   "id": "c4c053fecfa4c6b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sample_real_world_clamp(df_pool, size, min_per_class=MIN_PER_CLASS, random_state=None):\n",
    "    \"\"\"\n",
    "    Sample `size` rows reflecting df_pool's class frequencies,\n",
    "    ensuring at least `min_per_class` per class and exactly `size` total.\n",
    "    \"\"\"\n",
    "    counts = df_pool['update_classification'].value_counts()\n",
    "    total = counts.sum()\n",
    "\n",
    "    # Ideal allocations based on original frequencies\n",
    "    ideal = {c: counts[c] * size / total for c in counts.index}\n",
    "    alloc = {c: math.floor(ideal[c]) for c in counts.index}\n",
    "    frac = {c: ideal[c] - alloc[c] for c in counts.index}\n",
    "\n",
    "    # Distribute remaining slots by largest fractional parts\n",
    "    remaining = size - sum(alloc.values())\n",
    "    for c in sorted(frac, key=frac.get, reverse=True)[:remaining]:\n",
    "        alloc[c] += 1\n",
    "\n",
    "    # Enforce minimum and clamp to pool availability\n",
    "    for c in alloc:\n",
    "        avail = (df_pool['update_classification'] == c).sum()\n",
    "        alloc[c] = min(max(alloc[c], min_per_class), avail)\n",
    "\n",
    "    # If size exceeded due to min_per_class, remove surplus\n",
    "    total_alloc = sum(alloc.values())\n",
    "    if total_alloc > size:\n",
    "        surplus = total_alloc - size\n",
    "        # Reduce from classes above min_per_class, starting with those with highest original share\n",
    "        reducible = [c for c in counts.index if alloc[c] > min_per_class]\n",
    "        over = sorted(reducible, key=lambda c: counts[c], reverse=True)\n",
    "        idx = 0\n",
    "        while surplus > 0 and over:\n",
    "            c = over[idx % len(over)]\n",
    "            if alloc[c] > min_per_class:\n",
    "                alloc[c] -= 1\n",
    "                surplus -= 1\n",
    "            idx += 1\n",
    "\n",
    "    # Sample and shuffle\n",
    "    dfs = [\n",
    "        df_pool[df_pool['update_classification'] == c]\n",
    "               .sample(n=alloc[c], random_state=random_state)\n",
    "        for c in alloc\n",
    "    ]\n",
    "    return pd.concat(dfs).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "def sample_equal_clamp(df_pool, size, min_per_class=MIN_PER_CLASS, random_state=RANDOM_STATE):\n",
    "    \"\"\"\n",
    "    Sample `size` rows with equal class counts,\n",
    "    ensuring at least `min_per_class` per class and exactly `size` total.\n",
    "    \"\"\"\n",
    "    classes = df_pool['update_classification'].unique()\n",
    "    alloc = {c: min_per_class for c in classes}\n",
    "    remaining = size - sum(alloc.values())\n",
    "\n",
    "    # Distribute remaining slots round-robin until size reached\n",
    "    idx = 0\n",
    "    cls_list = list(classes)\n",
    "    while remaining > 0:\n",
    "        c = cls_list[idx % len(cls_list)]\n",
    "        avail = (df_pool['update_classification'] == c).sum()\n",
    "        if alloc[c] < avail:\n",
    "            alloc[c] += 1\n",
    "            remaining -= 1\n",
    "        idx += 1\n",
    "\n",
    "    # Sample and shuffle\n",
    "    dfs = [\n",
    "        df_pool[df_pool['update_classification'] == c]\n",
    "               .sample(n=alloc[c], random_state=random_state)\n",
    "        for c in alloc\n",
    "    ]\n",
    "    return pd.concat(dfs).sample(frac=1, random_state=random_state).reset_index(drop=True)"
   ],
   "id": "9289702c93833d5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Generate Training Sets",
   "id": "99094f1256f8a7a0"
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize containers for training sets\n",
    "d_training_real = {}\n",
    "d_training_equal = {}\n",
    "\n",
    "# For each specified training set size, create two pools (clamped to minimum per class)\n",
    "for size in TRAINING_SIZES:\n",
    "    df_real = sample_real_world_clamp(df_train_remaining, size, min_per_class=MIN_PER_CLASS, random_state=RANDOM_STATE)\n",
    "    df_equal = sample_equal_clamp(df_train_remaining, size,min_per_class=MIN_PER_CLASS, random_state=RANDOM_STATE)\n",
    "\n",
    "    d_training_real[size] = df_real\n",
    "    d_training_equal[size] = df_equal\n",
    "\n",
    "    print_dist(df_real, f\"Real-world training (n={size})\")\n",
    "    print_dist(df_equal, f\"Equal-dist training (n={size})\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f5a76221e10c14d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save Splits\n",
    "\n",
    "We save the validation and training splits to `DATA_DIR`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8d3a20197d666e4"
  },
  {
   "cell_type": "code",
   "source": [
    "# Save validation set\n",
    "val_path = os.path.join(DATA_DIR, \"demo_app_updates_validation_real_1000.csv\")\n",
    "df_val_final.to_csv(val_path, sep=\",\", index=False)\n",
    "\n",
    "# Save representative training sets\n",
    "for size, df_tr in d_training_real.items():\n",
    "    path = os.path.join(DATA_DIR, f\"demo_app_updates_train_real_{size}.csv\")\n",
    "    df_tr.to_csv(path, sep=\",\", index=False)\n",
    "\n",
    "# Save balanced training sets\n",
    "for size, df_tr in d_training_equal.items():\n",
    "    path = os.path.join(DATA_DIR, f\"demo_app_updates_train_equal_{size}.csv\")\n",
    "    df_tr.to_csv(path, sep=\",\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63ea135b215aeda",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation Fine-Tuning\n",
    "We prepare JSONL files for LLM APIs (OpenAI, Mistral) using the prompt from section 4.1.2 of the paper. The APIs of OpenAI and Mistral AI require inputs in .jsonl format. Files can be used for both APIs. Afterward, we validate each generated JSONL file for correct API use."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63d7f39def1d7950"
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the prompt\n",
    "PROMPT_TEMPLATE = f\"\"\"ONLY provide the category number (1-7) in response. Determine the category for the following app update text.\n",
    "If multiple categories seem applicable, always choose the lowest category number (1<2<3<4<5<6<7):\n",
    "\n",
    "(1) Novel features - Introducing or enhancing significant functionalities that modify user experience\n",
    "(2) Content extensions – Adding or expanding curated or user-facing content\n",
    "(3) Platform and device support – Enabling compatibility with new OS versions, development kits, device types, or hardware features\n",
    "(4) Specific fixes of bugs - Addressing distinct known issues\n",
    "(5) Privacy & Security - Strengthening user data protection, permissions, or security protocols\n",
    "(6) General Improvements – Tweaks addressing undisclosed bug fixes,  performance improvements, minor changes, or UI/UX polish\n",
    "(7) Marketing & Branding – Changes purely about promotional or visual branding assets\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87c9e86d43a14a02",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Helper function to create jsonl files for datasets in DATA_DIR\n",
    "def csv_to_jsonl(csv_path: str, jsonl_path: str, PROMPT_TEMPLATE: str) -> None:\n",
    "    \"\"\"\n",
    "    Read a CSV, build prompts, and write out a JSONL file\n",
    "    where each line is:\n",
    "      {\n",
    "        \"messages\": [\n",
    "          {\"role\":\"user\",      \"content\": <prompt>},\n",
    "          {\"role\":\"assistant\", \"content\": <label>}\n",
    "        ]\n",
    "      }\n",
    "    \"\"\"\n",
    "    # Read CSV\n",
    "    df_in = pd.read_csv(\n",
    "        csv_path,\n",
    "        dtype={'id': str, 'whats_new': str, 'update_classification': int},\n",
    "        parse_dates=['release_date'],\n",
    "        low_memory=False\n",
    "    )\n",
    "\n",
    "    # Build prompt column\n",
    "    df_in['prompt'] = PROMPT_TEMPLATE + \"\\n\\nApp update text: \" + df_in['whats_new']\n",
    "\n",
    "    # Assemble JSONL entries\n",
    "    with open(jsonl_path, 'w', encoding='utf-8') as fout:\n",
    "        for _, row in df_in.iterrows():\n",
    "            record = {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\",      \"content\": row.prompt},\n",
    "                    {\"role\": \"assistant\", \"content\": str(row.update_classification)}\n",
    "                ]\n",
    "            }\n",
    "            fout.write(json.dumps(record) + '\\n')\n",
    "\n",
    "\n",
    "# Loop over every CSV in the folder\n",
    "pattern = os.path.join(DATA_DIR, \"demo_app_updates_train_*.csv\")\n",
    "for csv_path in glob.glob(pattern):\n",
    "    base = os.path.splitext(os.path.basename(csv_path))[0]\n",
    "    jsonl_name = f\"{base}.jsonl\"\n",
    "    jsonl_path = os.path.join(LLM_API_FOLDER, jsonl_name)\n",
    "\n",
    "    print(f\"Converting {os.path.basename(csv_path)} → {jsonl_name}\")\n",
    "    csv_to_jsonl(csv_path, jsonl_path, PROMPT_TEMPLATE)\n",
    "\n",
    "print(\"All files processed.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4d888593201d3a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Helper function to validate jsonl files for API usage\n",
    "def check_jsonl_file(jsonl_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Load a JSONL file, print one example, and report any format errors.\n",
    "    \"\"\"\n",
    "    # Load all lines into Python objects\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        dataset = [json.loads(line) for line in f]\n",
    "\n",
    "    # Print header and one example\n",
    "    print(f\"Checking {os.path.basename(jsonl_path)}\")\n",
    "    print(\"Num examples:\", len(dataset))\n",
    "    if dataset:\n",
    "        print(\"Example[0] messages:\")\n",
    "        for msg in dataset[0].get(\"messages\", []):\n",
    "            print(f\"  {msg}\")\n",
    "\n",
    "    # Initialize error counters\n",
    "    format_errors = defaultdict(int)\n",
    "\n",
    "    # Validate each example\n",
    "    for ex in dataset:\n",
    "        if not isinstance(ex, dict):\n",
    "            format_errors[\"data_type\"] += 1\n",
    "            continue\n",
    "\n",
    "        messages = ex.get(\"messages\")\n",
    "        if not isinstance(messages, list) or not messages:\n",
    "            format_errors[\"missing_or_empty_messages_list\"] += 1\n",
    "            continue\n",
    "\n",
    "        # Validate each message in the list\n",
    "        for message in messages:\n",
    "            # Required keys: role, content\n",
    "            if \"role\" not in message or \"content\" not in message:\n",
    "                format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "            # No unexpected keys\n",
    "            if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "                format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "            # Role must be one of the allowed set\n",
    "            role = message.get(\"role\")\n",
    "            if role not in (\"system\", \"user\", \"assistant\"):\n",
    "                format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "            # Content must be a nonempty string\n",
    "            content = message.get(\"content\")\n",
    "            if not isinstance(content, str) or content.strip() == \"\":\n",
    "                format_errors[\"missing_or_invalid_content\"] += 1\n",
    "\n",
    "        # Ensure there's exactly one assistant response\n",
    "        if not any(m.get(\"role\") == \"assistant\" for m in messages):\n",
    "            format_errors[\"missing_assistant_message\"] += 1\n",
    "\n",
    "    # Print summary of any errors found\n",
    "    if format_errors:\n",
    "        print(\"Found format errors:\")\n",
    "        for err, count in format_errors.items():\n",
    "            print(f\"  {err}: {count}\")\n",
    "    else:\n",
    "        print(\"No errors found.\")\n",
    "\n",
    "# Loop over all .jsonl files in the directory and run checks\n",
    "for filename in os.listdir(LLM_API_FOLDER):\n",
    "    if filename.lower().endswith('.jsonl'):\n",
    "        check_jsonl_file(os.path.join(LLM_API_FOLDER, filename))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "325ba615188b1688",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
