{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "From a large dataset covering the complete review history of all apps that have been listed at least once in the top-100 overall or category-specific rankings, we randomly sampled 4,000 reviews written in English.\n",
    "To establish ground truth for the sample pool, a member of the research team and a research assistant independently coded the reviews using a coding scheme developed through inductive coding by our team. This scheme comprises nine content dimensions (detailed in Section 4.2.2). A subset of n=1,000 review descriptions was independently coded by both raters to assess inter-rater reliability (observed agreement = 0.89, macro-average κ = 0.77), and any discrepancies were discussed.\n",
    "We generated two distinct random subsamples from the labeled pool of n=4,000 reviews: a validation set of n=1,000 reviews serving as a holdout, and a training set of n=2,000 reviews. Both subsamples were constructed to reflect a representative class distribution of the overall pool. The remaining n=1,000 reviews of the labeled pool serves to back additional analyses to explore variations in sample sizes and class distributions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4813c2e3e94a5bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Imports\n",
    " See `requirements.txt` for full dependency versions"
   ],
   "id": "74de57b5176a5e2"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from collections import defaultdict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Global Paths, Directories and Variables",
   "id": "f027c630d98a86d8"
  },
  {
   "cell_type": "code",
   "source": [
    "# Define Demo Study path\n",
    "DEMO_PATH = os.path.abspath(os.path.join(\"..\"))\n",
    "\n",
    "# Define relevant paths \n",
    "DATA_DIR = os.path.join(DEMO_PATH, 'training_validation_data')\n",
    "LLM_API_FOLDER = os.path.join(DEMO_PATH, 'LLM_API')\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 94032"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b85c5b8a4199fb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Helper function for class distribution\n",
    "def print_dist(obj, name, classes=None):\n",
    "    \"\"\"\n",
    "    Print the count and percentage distribution of 'label' for the given DataFrame:\n",
    "        - If Series: interpreted as flat label list (e.g., [0, 2, 2, 5, ...])\n",
    "        - If ndarray: expected shape (n_examples, n_classes) with binary 0/1 entries (multi-label)\n",
    "    \"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        # Case: multi-label hot matrix (e.g., 2D [0/1] array)\n",
    "        if classes is None:\n",
    "            raise ValueError(\"print_dist: must pass classes for ndarray input\")\n",
    "        counts = obj.sum(axis=0)\n",
    "        pct    = (counts / obj.shape[0] * 100).round(2)\n",
    "        df     = pd.DataFrame({'count': counts, 'pct': pct}, index=classes)\n",
    "        print(f\"{name} distribution (n={obj.shape[0]} rows):\")\n",
    "        print(df, \"\\n\")\n",
    "\n",
    "    elif isinstance(obj, pd.Series):\n",
    "        # Case: single-label flat vector\n",
    "        dist = obj.value_counts().sort_index()\n",
    "        pct  = (dist / len(obj) * 100).round(2)\n",
    "        df   = pd.DataFrame({'count': dist, 'pct': pct})\n",
    "        print(f\"{name} distribution (n={len(obj)} labels):\")\n",
    "        print(df, \"\\n\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"print_dist: unsupported type {type(obj)}\")"
   ],
   "id": "90121b4ab4dd3e8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Pool\n",
    "\n",
    "We load the manually labeled dataset from the CSV file located in `DATA_DIR`. The dataset includes review (`title`) and (`body`), their IDs and assigned labels (`label`)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b236fb1f17e9573e"
  },
  {
   "cell_type": "code",
   "source": [
    "# Load manually labeled data pool for training and validation splits for model fine-tuning and performance evaluation \n",
    "df = pd.read_csv(os.path.join(DATA_DIR, \"02_demo_reviews_all_labeled.csv\"),\n",
    "    sep=\",\",\n",
    "    dtype={'review_id': str, 'user_id': str, 'title': str, 'body':str, 'label': str, 'id': str, 'app_id':str},\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "# Plain merge for ML pipelines\n",
    "df[\"review_text_plain\"] = (df[\"title\"].fillna(\"\").str.strip() + \" \" + df[\"body\"].fillna(\"\").str.strip())\n",
    "\n",
    "# Tagged merge for LLMs\n",
    "df[\"review_text_tagged\"] = (\n",
    "    \"Title: \" + df[\"title\"].fillna(\"\").str.strip()\n",
    "    + \"\\nBody: \" + df[\"body\"].fillna(\"\").str.strip()\n",
    ")\n",
    "\n",
    "# Clean and prepare labels\n",
    "df[\"label\"] = df[\"label\"].str.strip()\n",
    "df[\"split_labels\"] = (\n",
    "    df[\"label\"]\n",
    "      .apply(lambda x: [lbl.strip() for lbl in x.split(\";\") if lbl.strip().isdigit()])\n",
    "      .apply(lambda lst: sorted(set(lbl for lbl in lst if 0 <= int(lbl) <= 8)))\n",
    ") # Split on \";\" and keep only digits 0–8, sorted & unique\n",
    "df[\"sorted_labels\"] = df[\"split_labels\"].apply(lambda lst: \";\".join(lst))\n",
    "\n",
    "# Binarize all labels\n",
    "mlb    = MultiLabelBinarizer(classes=[str(i) for i in range(9)])\n",
    "Y_all  = mlb.fit_transform(df[\"split_labels\"])\n",
    "CLASSES = mlb.classes_\n",
    "\n",
    "# Class Distribution Overview\n",
    "all_lbls = [lbl for sub in df[\"split_labels\"] for lbl in sub]\n",
    "print_dist(pd.Series(all_lbls), \"Single-Label\") # Single-label distribution\n",
    "\n",
    "print_dist(df[\"sorted_labels\"], \"Multi-Label Combo\") # Multi-label combination distribution"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2f1dcd7e8941e58",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validation and Training Data Splits\n",
    "We generate random subsamples from the labeled sample pool: a validation set of n = 1,000 reviews serving as a holdout and training sets with varying sample sizes of n = 2,000; 1,000; 500; 250; 100 reviews. Subsamples were constructed to reflect both a class distribution similar to that of the overall pool (representative) and, when possible, an equally balanced class distribution."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "953a1aba96c341b3"
  },
  {
   "cell_type": "code",
   "source": [
    "# Define constants (here set based on data pool class distribution)\n",
    "VAL_SIZE        = 1000      # Number of validation samples to hold out\n",
    "MIN_PER_CLASS   = 3         # Minimum labeled texts per class when sampling training data\n",
    "TRAINING_SIZES  = [2000, 1000, 500, 250, 100]  # Various training set sizes to generate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eed36fe7584270e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Validation Set Creation\n",
    "\n",
    "We create a validation set of size `VAL_SIZE`. The remaining data forms the training pool."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b1d93b1987ad985"
  },
  {
   "cell_type": "code",
   "source": [
    "# Build index array for splitting\n",
    "indices = np.arange(len(df)).reshape(-1, 1)\n",
    "\n",
    "msss_val = MultilabelStratifiedShuffleSplit(\n",
    "    n_splits=1,\n",
    "    test_size=VAL_SIZE,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "train_idx, val_idx = next(msss_val.split(indices, Y_all))\n",
    "\n",
    "df_val        = df.iloc[val_idx].reset_index(drop=True)\n",
    "Y_val         = Y_all[val_idx]\n",
    "df_train_pool = df.iloc[train_idx].reset_index(drop=True)\n",
    "Y_train_pool  = Y_all[train_idx]\n",
    "\n",
    "# Single‐label distribution in the validation set\n",
    "all_lbls_val = [lbl for sub in df_val[\"split_labels\"] for lbl in sub]\n",
    "print_dist(pd.Series(all_lbls_val), \"Validation Single‐Label\")\n",
    "\n",
    "# Multi‐label‐combo distribution in the validation set\n",
    "print_dist(df_val[\"sorted_labels\"], \"Validation Multi‐Label Combo\")\n",
    "\n",
    "print(f\"Training pool size (n={len(df_train_pool)})\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b853f74f71aa4199",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training Set Creation\n",
    "\n",
    "For each size in `TRAINING_SIZES`, we draw a stratified subset from the training pool, both real-world weighted and equal-distribution."
   ],
   "id": "aa76fa4c009a9b33"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Sampling Functions\n",
    "\n",
    "We define helper functions to sample training subsets either reflecting representative (real-world) distributions or balanced (equal) class distribution, with at least `MIN_PER_CLASS` samples per class."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a1b8c1406151bd3"
  },
  {
   "cell_type": "code",
   "source": [
    "def sample_real_world_clamp(df_pool,\n",
    "                           size,\n",
    "                           label_col='split_labels',\n",
    "                           classes=[str(i) for i in range(9)],\n",
    "                           min_per_class=MIN_PER_CLASS,\n",
    "                           random_state=RANDOM_STATE):\n",
    "    \"\"\"\n",
    "    Sample `size` rows from a multi-label pool, roughly preserving label frequencies,\n",
    "    while ensuring each label appears at least `min_per_class` times.\n",
    "\n",
    "    Uses MultilabelStratifiedShuffleSplit for initial approximation and clamps final result.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "\n",
    "    # Transform multilabel column to binary indicator matrix\n",
    "    Y = mlb.transform(df_pool[label_col])\n",
    "    msss = MultilabelStratifiedShuffleSplit(\n",
    "        n_splits=1, test_size=size, random_state=random_state\n",
    "    )\n",
    "    _, idx = next(msss.split(np.zeros((len(df_pool),1)), Y))\n",
    "    sel = set(idx)\n",
    "\n",
    "    # Track current label counts in selection\n",
    "    counts = {lbl: 0 for lbl in classes}\n",
    "    for i in sel:\n",
    "        for lbl in df_pool.at[i, label_col]:\n",
    "            counts[lbl] += 1\n",
    "\n",
    "    # Add missing labels to reach min_per_class\n",
    "    for lbl in classes:\n",
    "        short = max(0, min_per_class - counts[lbl])\n",
    "        if short:\n",
    "            mask = df_pool[label_col].apply(lambda L: lbl in L)\n",
    "            pool = [i for i in df_pool.index[mask] if i not in sel]\n",
    "            take = min(short, len(pool))\n",
    "            chosen = rng.choice(pool, take, replace=False)\n",
    "            sel.update(chosen)\n",
    "            counts[lbl] += take\n",
    "\n",
    "    # If selection too large, remove 'safe' rows first\n",
    "    if len(sel) > size:\n",
    "        need_rm = len(sel) - size\n",
    "        removable = [\n",
    "            i for i in sel\n",
    "            if all(counts[l] - 1 >= min_per_class \n",
    "                   for l in df_pool.at[i, label_col])\n",
    "        ]\n",
    "\n",
    "        if len(removable) >= need_rm:\n",
    "            to_remove = list(rng.choice(removable, need_rm, replace=False))\n",
    "        else:\n",
    "            to_remove = list(removable)\n",
    "            rest = list(sel - set(removable))\n",
    "            extra = rng.choice(rest, need_rm - len(removable), replace=False)\n",
    "            to_remove.extend(extra)\n",
    "\n",
    "        for i in to_remove:\n",
    "            sel.remove(i)\n",
    "            for lbl in df_pool.at[i, label_col]:\n",
    "                counts[lbl] -= 1\n",
    "\n",
    "    # If still short, randomly fill up to target size\n",
    "    if len(sel) < size:\n",
    "        remaining = list(set(df_pool.index) - sel)\n",
    "        need = size - len(sel)\n",
    "        added = rng.choice(remaining, need, replace=False)\n",
    "        for i in added:\n",
    "            sel.add(i)\n",
    "            for lbl in df_pool.at[i, label_col]:\n",
    "                counts[lbl] += 1\n",
    "\n",
    "    # Final sample and shuffle\n",
    "    return (\n",
    "        df_pool.loc[list(sel)]\n",
    "               .sample(frac=1, random_state=random_state)\n",
    "               .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "def sample_equal_clamp(df_pool,\n",
    "                       size,\n",
    "                       label_col='split_labels',\n",
    "                       classes=CLASSES,\n",
    "                       min_per_class=MIN_PER_CLASS,\n",
    "                       random_state=RANDOM_STATE):\n",
    "    \"\"\"\n",
    "    Sample `size` rows with equal label representation across `classes`.\n",
    "    Each label appears at least `min_per_class` times, or size // n_labels, whichever is larger.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    sel = set()\n",
    "\n",
    "    # Target per label (equal allocation)\n",
    "    per_lbl = max(min_per_class, size // len(classes))\n",
    "\n",
    "    # Sample independently for each label\n",
    "    for lbl in classes:\n",
    "        idxs = df_pool.index[df_pool[label_col].apply(lambda L: lbl in L)]\n",
    "        k    = min(per_lbl, len(idxs))\n",
    "        if k:\n",
    "            chosen = rng.choice(idxs, k, replace=False)\n",
    "            sel |= set(chosen)\n",
    "\n",
    "    # Track current label counts\n",
    "    counts = {lbl: 0 for lbl in classes}\n",
    "    for i in sel:\n",
    "        for lbl in df_pool.at[i, label_col]:\n",
    "            counts[lbl] += 1\n",
    "\n",
    "    # If selection too large, remove 'safe' rows first\n",
    "    if len(sel) > size:\n",
    "        need_rm = len(sel) - size\n",
    "        removable = [\n",
    "            i for i in sel\n",
    "            if all(counts[l] - 1 >= min_per_class\n",
    "                   for l in df_pool.at[i, label_col])\n",
    "        ]\n",
    "\n",
    "        if len(removable) >= need_rm:\n",
    "            to_remove = rng.choice(removable, need_rm, replace=False)\n",
    "        else:\n",
    "            to_remove = list(removable)\n",
    "            rest = list(sel - set(removable))\n",
    "            extra = rng.choice(rest, need_rm - len(removable), replace=False)\n",
    "            to_remove.extend(extra)\n",
    "\n",
    "        for i in to_remove:\n",
    "            sel.remove(i)\n",
    "            for lbl in df_pool.at[i, label_col]:\n",
    "                counts[lbl] -= 1\n",
    "\n",
    "    # If still short, randomly fill up to target size\n",
    "    if len(sel) < size:\n",
    "        remaining = list(set(df_pool.index) - sel)\n",
    "        need = size - len(sel)\n",
    "        added = rng.choice(remaining, need, replace=False)\n",
    "        for i in added:\n",
    "            sel.add(i)\n",
    "            for lbl in df_pool.at[i, label_col]:\n",
    "                counts[lbl] += 1\n",
    "\n",
    "    # Final sample and shuffle\n",
    "    return (\n",
    "        df_pool.loc[list(sel)]\n",
    "               .sample(frac=1, random_state=random_state)\n",
    "               .reset_index(drop=True)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbef156c4052abb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Generate Multi-Label Stratified Training Sets",
   "id": "8c50478b2ba6b9b9"
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize containers for training sets\n",
    "d_real  = {}\n",
    "d_equal = {}\n",
    "\n",
    "# For each specified training set size, create two pools (clamped to minimum per class)\n",
    "for size in TRAINING_SIZES:\n",
    "    real_df = sample_real_world_clamp(df_train_pool, size)\n",
    "    y_real  = mlb.transform(real_df[\"split_labels\"])\n",
    "    d_real[size] = (real_df, y_real)\n",
    "\n",
    "    eq_df   = sample_equal_clamp(df_train_pool, size)\n",
    "    y_eq    = mlb.transform(eq_df[\"split_labels\"])\n",
    "    d_equal[size] = (eq_df, y_eq)\n",
    "\n",
    "    # now a single call each\n",
    "    print_dist(y_real, name=f\"Real-world single-label\", classes=CLASSES)\n",
    "    print_dist(y_eq, name=f\"Approx-equal single-label\", classes=CLASSES)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f09841ac4c9732f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Save Splits\n",
    "\n",
    "We save the validation and training splits to `DATA_DIR`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "612e6676c4467644"
  },
  {
   "cell_type": "code",
   "source": [
    "# Save validation set\n",
    "df_val[\"multi_hot\"] = [json.dumps(vec.tolist()) for vec in Y_val] # convert each row’s vector to a JSON string\n",
    "df_val.to_csv(\n",
    "    os.path.join(DATA_DIR, \"demo_product_reviews_validation_real_1000.csv\"),\n",
    "    index=False,\n",
    "    quoting=csv.QUOTE_MINIMAL\n",
    ")\n",
    "\n",
    "# Save representative training sets\n",
    "for size, (df_tr, y_tr) in d_real.items():\n",
    "    # turn inner numpy arrays into Python lists, then dump to JSON\n",
    "    df_tr[\"multi_hot\"] = [json.dumps(vec.tolist()) for vec in y_tr]\n",
    "    df_tr.to_csv(\n",
    "        os.path.join(DATA_DIR, f\"demo_product_reviews_train_real_{size}.csv\"),\n",
    "        index=False,\n",
    "        quoting=csv.QUOTE_MINIMAL\n",
    "    )\n",
    "\n",
    "# Save balanced training sets\n",
    "for size, (df_tr, y_tr) in d_equal.items():\n",
    "    df_tr[\"multi_hot\"] = [json.dumps(vec.tolist()) for vec in y_tr]\n",
    "    df_tr.to_csv(\n",
    "        os.path.join(DATA_DIR, f\"demo_product_reviews_train_equal_{size}.csv\"),\n",
    "        index=False,\n",
    "        quoting=csv.QUOTE_MINIMAL\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c6186353c19f19f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation Fine-Tuning\n",
    "We prepare JSONL files for LLM APIs (OpenAI, Mistral) using the prompt from section 4.2.2 of the paper. The APIs of OpenAI and Mistral AI require inputs in .jsonl format. Files can be used for both APIs. Afterward, we validate each generated JSONL file for correct API use."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "774fc74ba025bd23"
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the prompt\n",
    "PROMPT_TEMPLATE = f\"\"\"ONLY provide a number (0-8) in response. Categorize the following app review text by assigning the most fitting category/categories out of the following nine categories.\n",
    "If the text contains elements from multiple categories, provide the categories separated by ;\n",
    "\n",
    "(0)\tUser Opinion without specific reports, issues, suggestions - i.e., review only about good/bad perception of the app but nothing else\n",
    "(1)\tReports of bugs, errors, or bad quality issues - i.e., something does not work in the app or is of bad quality\n",
    "(2)\tIssues of the app's monetization model - i.e., complaints or issues of how the app monetizes content\n",
    "(3)\tSuggestions for new features or content or revival of removed features\n",
    "(4)\tCustomer support issues - i.e., problems or complaints regarding customer support\n",
    "(5)\tPerformance issues - i.e., the app needs to much space, is to slow or similar\n",
    "(6)\tSecurity concerns - i.e., user is concerned about their data or privacy\n",
    "(7)\tEthical concerns - i.e., user is concerned about practices in the app, fairness, discrimination\n",
    "(8)\tCommunity related issues - user asks openly for help (no feedback at customer support)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1aa200e8f5774dd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Helper function to create jsonl files for datasets in DATA_DIR\n",
    "def csv_to_jsonl(csv_path: str, jsonl_path: str, PROMPT_TEMPLATE: str) -> None:\n",
    "    \"\"\"\n",
    "    Read a CSV, build prompts, and write out a JSONL file\n",
    "    where each line is:\n",
    "      {\n",
    "        \"messages\": [\n",
    "          {\"role\":\"user\",      \"content\": <prompt>},\n",
    "          {\"role\":\"assistant\", \"content\": <label>}\n",
    "        ]\n",
    "      }\n",
    "    \"\"\"\n",
    "    # Read CSV\n",
    "    df_in = pd.read_csv(csv_path,\n",
    "        sep=\",\",\n",
    "        header=0,\n",
    "        dtype={'review_id': str, 'user_id': str, 'title': str, 'body':str, 'review_text_tagged':str, 'label': str, 'id': str, 'app_id':str},\n",
    "        low_memory=False\n",
    "    )\n",
    "\n",
    "    # Merge title & body\n",
    "    df_in['app_review'] = df_in['review_text_tagged'].fillna('')\n",
    "\n",
    "    # Build the prompt column\n",
    "    df_in['prompt'] = PROMPT_TEMPLATE + \"\\n\\nApp review text: \" + df_in['app_review']\n",
    "\n",
    "    # Assemble JSONL entries\n",
    "    with open(jsonl_path, 'w', encoding='utf-8') as fout:\n",
    "        for _, row in df_in.iterrows():\n",
    "            record = {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\",      \"content\": row.prompt},\n",
    "                    {\"role\": \"assistant\", \"content\": str(row.label)}\n",
    "                ]\n",
    "            }\n",
    "            fout.write(json.dumps(record) + '\\n')\n",
    "\n",
    "# Loop over every CSV in the folder\n",
    "pattern = os.path.join(DATA_DIR, \"demo_product_reviews_train_*.csv\")\n",
    "for csv_path in glob.glob(pattern):\n",
    "    base = os.path.splitext(os.path.basename(csv_path))[0]\n",
    "    jsonl_name = f\"{base}.jsonl\"\n",
    "    jsonl_path = os.path.join(LLM_API_FOLDER, jsonl_name)\n",
    "\n",
    "    print(f\"Converting {os.path.basename(csv_path)} → {jsonl_name}...\")\n",
    "    csv_to_jsonl(csv_path, jsonl_path, PROMPT_TEMPLATE)\n",
    "\n",
    "print(\"All files processed.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5af40eec95638f2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Helper function to validate jsonl files for API usage\n",
    "def check_jsonl_file(jsonl_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Load a JSONL file, print one example, and report any format errors.\n",
    "    \"\"\"\n",
    "    # Load all lines into Python objects\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        dataset = [json.loads(line) for line in f]\n",
    "\n",
    "    # Print header and one example\n",
    "    print(f\"Checking {os.path.basename(jsonl_path)}\")\n",
    "    print(\"Num examples:\", len(dataset))\n",
    "    if dataset:\n",
    "        print(\"Example[0] messages:\")\n",
    "        for msg in dataset[0].get(\"messages\", []):\n",
    "            print(f\"  {msg}\")\n",
    "\n",
    "    # Initialize error counters\n",
    "    format_errors = defaultdict(int)\n",
    "\n",
    "    # Validate each example\n",
    "    for ex in dataset:\n",
    "        if not isinstance(ex, dict):\n",
    "            format_errors[\"data_type\"] += 1\n",
    "            continue\n",
    "\n",
    "        messages = ex.get(\"messages\")\n",
    "        if not isinstance(messages, list) or not messages:\n",
    "            format_errors[\"missing_or_empty_messages_list\"] += 1\n",
    "            continue\n",
    "\n",
    "        # Validate each message in the list\n",
    "        for message in messages:\n",
    "            # Required keys: role, content\n",
    "            if \"role\" not in message or \"content\" not in message:\n",
    "                format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "            # No unexpected keys\n",
    "            if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "                format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "            # Role must be one of the allowed set\n",
    "            role = message.get(\"role\")\n",
    "            if role not in (\"system\", \"user\", \"assistant\"):\n",
    "                format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "            # Content must be a nonempty string\n",
    "            content = message.get(\"content\")\n",
    "            if not isinstance(content, str) or content.strip() == \"\":\n",
    "                format_errors[\"missing_or_invalid_content\"] += 1\n",
    "\n",
    "        # Ensure there's exactly one assistant response\n",
    "        if not any(m.get(\"role\") == \"assistant\" for m in messages):\n",
    "            format_errors[\"missing_assistant_message\"] += 1\n",
    "\n",
    "    # Print summary of any errors found\n",
    "    if format_errors:\n",
    "        print(\"Found format errors:\")\n",
    "        for err, count in format_errors.items():\n",
    "            print(f\"  {err}: {count}\")\n",
    "    else:\n",
    "        print(\"No errors found.\")\n",
    "\n",
    "# Loop over all .jsonl files in the directory and run checks\n",
    "for filename in os.listdir(LLM_API_FOLDER):\n",
    "    if filename.lower().endswith('.jsonl'):\n",
    "        check_jsonl_file(os.path.join(LLM_API_FOLDER, filename))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5d1e37416487469",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
