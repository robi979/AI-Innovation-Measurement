{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "In this notebook, we generate all tables and figures based on the results of demonstration study 2, for both the main paper and the online appendix. This includes data filtering, statistical summaries, and all visualization scripts. All output files are saved to the `paper_visuals` and `output_data` directories."
   ],
   "id": "959d6f6da55c5087"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Imports\n",
    " See `requirements.txt` for full dependency versions"
   ],
   "id": "7eeb7a6e9e96f2db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.metrics import f1_score"
   ],
   "id": "72bc10f70d2a9ee3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Global Variables, Paths, and Settings",
   "id": "c1d9a184033cede6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define Demo Study path\n",
    "DEMO_PATH   = os.path.abspath(os.path.join(\"..\"))\n",
    "\n",
    "# Define relevant paths\n",
    "OUT_DIR     = os.path.join(DEMO_PATH, \"output_data\")\n",
    "LLM_PATH    = os.path.join(OUT_DIR, \"validation_with_model_preds_LLM_cleaned.csv\")\n",
    "MODELS_PATH = os.path.join(OUT_DIR, \"validation_ovr_with_model_preds_NLP.csv\")\n",
    "VISUAL_DIR  = os.path.join(DEMO_PATH, \"paper_visuals\")\n",
    "\n",
    "# Define number of labels\n",
    "NUM_LABELS = 9"
   ],
   "id": "5a1622310f1ba0e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pre-Processing\n",
    "In this section, we define regular expressions and helper functions for parsing model output columns. This includes extracting metadata (e.g., data type, size, classifier/model, and vectorizer) for classical ML, CNN, and PLM models, as well as standardizing column names and processing LLM results. These routines are used to prepare the results tables and figures for the paper and online appendix."
   ],
   "id": "59e4e9cad0db9872"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define constants for parsing model output column names and metadata extraction\n",
    "DATA_TYPES   = ['real', 'equal']\n",
    "DATA_SIZES   = ['2000','1000','500','250','100']\n",
    "CLASSIFIERS  = ['RandomForest','KNeighbors','LogisticRegression',\n",
    "                'NaiveBayes','SVM','XGBoost']\n",
    "CNN_MODELS   = ['TextCNN42B']\n",
    "PLM_MODELS   = ['roberta','bert','electra','xlnet']\n",
    "GLOVE_VECTORIZERS = [\"6B-100d\",\"6B-300d\",\"42B-300d\",\"840B-300d\"]\n",
    "\n",
    "# Pre-compiled regex patterns to match output columns for each model family\n",
    "ML_PATTERN    = re.compile(\n",
    "    rf'^(.+?)__({\"|\".join(DATA_TYPES)})__({\"|\".join(DATA_SIZES)})__'\n",
    "    rf'({\"|\".join(CLASSIFIERS)})_pred$', re.I)\n",
    "CNN_PATTERN   = re.compile(\n",
    "    rf'^({\"|\".join(CNN_MODELS)})__({\"|\".join(DATA_TYPES)})__'\n",
    "    rf'({\"|\".join(DATA_SIZES)})_pred$', re.I)\n",
    "PLM_PATTERN   = re.compile(\n",
    "    rf'^({\"|\".join(PLM_MODELS)})_({\"|\".join(DATA_TYPES)})_('\n",
    "    rf'{\"|\".join(DATA_SIZES)})_pred$', re.I)\n",
    "GLOVE_PATTERN = re.compile(\n",
    "    rf'^({\"|\".join(GLOVE_VECTORIZERS)})_({\"|\".join(DATA_TYPES)})_('\n",
    "    rf'{\"|\".join(DATA_SIZES)})__({\"|\".join(CLASSIFIERS)})_pred$', re.I)\n",
    "\n",
    "# Define known LLM and other model keys for later identification and provider mapping\n",
    "MODEL_KEYS = sorted([\n",
    "    # OpenAI\n",
    "    \"gpt_4_1_nano\", \"gpt_4_1_mini\", \"gpt_4_1\",\n",
    "    # Mistral\n",
    "    \"mistral_large\", \"mistral_medium\", \"mistral_small\",\n",
    "    \"open_mistral_nemo\", \"ministral_8b\", \"ministral_3b\",\n",
    "    # Anthropic\n",
    "    \"4_sonnet\", \"sonnet_4\",\n",
    "    \"3_5_haiku\", \"3_haiku\",\n",
    "    \"3_7_sonnet\", \"3_5_sonnet\",\n",
    "    \"3_opus\", \"opus_4\",\n",
    "    # OpenAI (legacy keys)\n",
    "    \"gpt_4o_mini\", \"gpt_4o\",\n",
    "    \"gpt_3_5_turbo_0125\",\n",
    "    \"o4_mini_2025_04_16\", \"o3_2025_04_16\",\n",
    "], key=len, reverse=True)\n",
    "\n",
    "# Mapping model keys to their provider (OpenAI, Mistral, Anthropic)\n",
    "PROVIDER = {\n",
    "    **{k:\"OpenAI\"   for k in MODEL_KEYS if k.startswith(\"gpt\") or k.startswith(\"o\")},\n",
    "    **{k:\"Mistral\"  for k in MODEL_KEYS if \"mistral\" in k or \"ministral\" in k},\n",
    "    **{k:\"Anthropic\"for k in MODEL_KEYS if k.endswith((\"haiku\",\"sonnet\",\"opus\"))},\n",
    "}"
   ],
   "id": "7cc45cc34103731c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Helper functions for column and prediction parsing",
   "id": "fb0897c06f0acfc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Model metadata parsing helpers\n",
    "def standardize_llm_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardizes LLM column names by removing trailing '__review' and fixing underscores\n",
    "    before temperature tokens (e.g., '_t0' → '__t0').\n",
    "    \"\"\"\n",
    "    def _fix(c):\n",
    "        c = re.sub(r'__review$', '', c)\n",
    "        return re.sub(r'(?<!_)_(t\\d+)', r'__\\1', c)\n",
    "    df = df.copy()\n",
    "    df.columns = [_fix(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def parse_ml_info(col: str) -> dict|None:\n",
    "    \"\"\"\n",
    "    Parses ML/CNN/PLM column names to extract model metadata.\n",
    "    Returns a dictionary of metadata if a pattern matches, otherwise None.\n",
    "    \"\"\"\n",
    "    m = GLOVE_PATTERN.match(col)\n",
    "    if m:\n",
    "        vec, dist, size, clf = m.groups()\n",
    "        return dict(model=clf, category=\"ML\", vectorizer=vec,\n",
    "                    distribution=dist, size=int(size))\n",
    "    m = ML_PATTERN.match(col)\n",
    "    if m:\n",
    "        vec, dist, size, clf = m.groups()\n",
    "        return dict(model=clf, category=\"ML\", vectorizer=vec,\n",
    "                    distribution=dist, size=int(size))\n",
    "    m = CNN_PATTERN.match(col)\n",
    "    if m:\n",
    "        model, dist, size = m.groups()\n",
    "        return dict(model=model, category=\"CNN\", distribution=dist, size=int(size))\n",
    "    m = PLM_PATTERN.match(col)\n",
    "    if m:\n",
    "        model, dist, size = m.groups()\n",
    "        return dict(model=model, category=\"PLM\", distribution=dist, size=int(size))\n",
    "    return None\n",
    "\n",
    "def parse_llm_info(prefix: str) -> dict|None:\n",
    "    \"\"\"\n",
    "    Parses LLM column prefixes to extract metadata:\n",
    "    model, provider, prompt type, temperature, fine-tuning flag, etc.\n",
    "    Returns a dictionary if successfully parsed, else None.\n",
    "    \"\"\"\n",
    "    parts = prefix.split(\"__\")\n",
    "    if len(parts) != 3:\n",
    "        return None\n",
    "\n",
    "    core, prompt_type, temp_str = parts\n",
    "    fine_tuned = core.startswith(\"ft_\")\n",
    "    if fine_tuned:\n",
    "        core = core[3:]\n",
    "\n",
    "    # Look up model and provider\n",
    "    model = next((mk for mk in MODEL_KEYS if mk in core), None)\n",
    "    provider = PROVIDER.get(model)\n",
    "\n",
    "    # Extract distribution and size if present\n",
    "    m_ds = re.search(rf\"_(real|equal)_({'|'.join(DATA_SIZES)})\", core)\n",
    "    distribution, size = (m_ds.group(1), int(m_ds.group(2))) if m_ds else (None, None)\n",
    "\n",
    "    # Parse temperature value (e.g., t0 → 0.0)\n",
    "    temperature = None\n",
    "    if temp_str.startswith(\"t\") and temp_str[1:].isdigit():\n",
    "        temperature = int(temp_str[1:]) / 10.0\n",
    "\n",
    "    return {\n",
    "        \"vectorizer\":   None,\n",
    "        \"distribution\": distribution,\n",
    "        \"size\":         size,\n",
    "        \"model\":        model or core,   # fallback if unseen\n",
    "        \"category\":     \"LLM\",\n",
    "        \"provider\":     provider,\n",
    "        \"prompt_type\":  prompt_type,\n",
    "        \"temperature\":  temperature,\n",
    "        \"fine_tuned\":   int(fine_tuned),\n",
    "    }\n",
    "\n",
    "# Multilabel prediction parsing helpers\n",
    "def parse_multi_hot(val, num_labels=NUM_LABELS):\n",
    "    \"\"\"Convert a prediction (list/array/str) to a multi-hot label vector.\"\"\"\n",
    "    if isinstance(val, (list, np.ndarray)):\n",
    "        return [int(x) for x in val]\n",
    "    if isinstance(val, str):\n",
    "        return [int(x) for x in re.findall(r'\\d', val)]\n",
    "    return [0]*num_labels\n",
    "\n",
    "def parse_model_pred(val, num_labels=NUM_LABELS):\n",
    "    \"\"\"\n",
    "    Parse LLM or model prediction as a multi-hot vector.\n",
    "    Handles empty, 'nan', or '9' (all-negative) cases robustly.\n",
    "    \"\"\"\n",
    "    val = str(val).strip()\n",
    "    if val in (\"\", \"9\", \"nan\"): return [0]*num_labels\n",
    "    vec = [0]*num_labels\n",
    "    for tok in val.replace(\" \", \"\").split(\";\"):\n",
    "        if tok.isdigit() and 0 <= int(tok) < num_labels:\n",
    "            vec[int(tok)] = 1\n",
    "    return vec\n",
    "\n",
    "# Metric computation helpers\n",
    "def multilabel_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute multilabel F1 (macro/micro/weighted), subset accuracy, and hamming loss.\n",
    "    Returns a dictionary of metrics for result tables.\n",
    "    \"\"\"\n",
    "    f1s = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    return {\n",
    "        **{f\"f1_{i}\": f for i, f in enumerate(f1s)},\n",
    "        \"f1_macro\":    f1_score(y_true, y_pred, average=\"macro\",   zero_division=0),\n",
    "        \"f1_micro\":    f1_score(y_true, y_pred, average=\"micro\",   zero_division=0),\n",
    "        \"f1_weighted\": f1_score(y_true, y_pred, average=\"weighted\",zero_division=0),\n",
    "        \"subset_acc\":  np.mean((y_true == y_pred).all(axis=1)),\n",
    "        \"hamming_loss\":np.mean(y_true != y_pred),\n",
    "    }\n",
    "\n",
    "def consistency_across_runs(df, cols):\n",
    "    \"\"\"\n",
    "    Compute the fraction of samples for which all prediction runs agree (consistency).\n",
    "    Used to assess LLM stability across repeated calls.\n",
    "    \"\"\"\n",
    "    arrs = [np.vstack(df[c].apply(parse_model_pred).values) for c in cols]\n",
    "    arrs = np.stack(arrs, axis=1)\n",
    "    return np.mean([np.all(arrs[i]==arrs[i][0]) for i in range(arrs.shape[0])])"
   ],
   "id": "117f5c55053b0f9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Main function for compiling results DataFrame for all models",
   "id": "8bd3e03f6b113e35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_multilabel_results(df_ml, df_llm) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a tidy DataFrame summarizing all multilabel metrics, consistency, and model metadata\n",
    "    for all predictors (ML, CNN, PLM, LLM). Used for generating result tables and figures.\n",
    "    \"\"\"\n",
    "    exclude = {'review_id','user_id','title','body','label','id','app_id',\n",
    "               'review_text_plain','review_text_tagged',\n",
    "               'split_labels','sorted_labels','multi_hot'}\n",
    "    records = []\n",
    "\n",
    "    # Ground-truth label arrays\n",
    "    y_true_ml  = np.vstack(df_ml['multi_hot'].apply(parse_multi_hot).values)\n",
    "    y_true_llm = np.vstack(df_llm['multi_hot'].apply(parse_multi_hot).values)\n",
    "\n",
    "    # ML / CNN / PLM models\n",
    "    for col in (c for c in df_ml.columns if c not in exclude):\n",
    "        info = parse_ml_info(col)\n",
    "        if not info: continue\n",
    "        y_pred = np.vstack(df_ml[col].apply(parse_multi_hot).values)\n",
    "        rec    = {**info, \"raw_name\":col,\n",
    "                  **multilabel_metrics(y_true_ml, y_pred),\n",
    "                  \"consistency\": np.nan}\n",
    "        records.append(rec)\n",
    "\n",
    "    # LLMs\n",
    "    llm_cols = [c for c in df_llm.columns if c not in exclude]\n",
    "    prefix_runs = {}\n",
    "    for c in llm_cols:\n",
    "        m = re.match(r\"(.+?)(__run\\d+|__r\\d+)$\", c)\n",
    "        prefix = m.group(1) if m else c\n",
    "        prefix_runs.setdefault(prefix, []).append(c)\n",
    "\n",
    "    for prefix, runs in prefix_runs.items():\n",
    "        info = parse_llm_info(prefix)\n",
    "        if not info: continue\n",
    "        main = sorted(runs)[0]\n",
    "        y_pred = np.vstack(df_llm[main].apply(parse_model_pred).values)\n",
    "        rec = {**info, \"raw_name\":main,\n",
    "               **multilabel_metrics(y_true_llm, y_pred),\n",
    "               \"consistency\": consistency_across_runs(df_llm, runs)\n",
    "                              if len(runs) > 1 else np.nan}\n",
    "        records.append(rec)\n",
    "\n",
    "    return pd.DataFrame.from_records(records)"
   ],
   "id": "7654c0ad93c53630",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Load Model Predictions and Compile Results Table\n",
    "We load the output prediction files for classical models (`MODELS_PATH`) and LLMs (`LLM_PATH`). We standardize column names, parse predictions, and compile a tidy results table containing all relevant evaluation metrics and metadata for each predictor. Processed results are saved in the `output_data` directory for downstream table and figure generation."
   ],
   "id": "877fab8e235d8869"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load prediction files\n",
    "df_models_raw = pd.read_csv(MODELS_PATH, low_memory=False)\n",
    "df_llm_raw    = pd.read_csv(LLM_PATH, low_memory=False)\n",
    "\n",
    "# Standardize LLM column names\n",
    "df_llm    = standardize_llm_columns(df_llm_raw)\n",
    "df_models = df_models_raw.copy()\n",
    "\n",
    "# Compile results for all models and LLMs\n",
    "results_df = build_multilabel_results(df_models, df_llm)\n",
    "results_df.to_csv(os.path.join(OUT_DIR, \"performance_reviews_label-specific.csv\"), index=False)"
   ],
   "id": "5b2c395bcd83a0ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Visuals\n",
    "In this section, we generate all tables (Tables 4, 5, and 6) and figures (Figures 6, 7, and C1) for the paper and online appendix, based on the processed results.\n"
   ],
   "id": "3880bb4c97df4e86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the processed results table from the previous steps\n",
    "df = pd.read_csv(os.path.join(OUT_DIR, \"performance_reviews_label-specific.csv\"))\n",
    "df = df[~((df[\"raw_name\"] == \"ft_mistral_small_latest_d1ef7e20_20250529_bba6ea02_real_250__default__t0__run1\") & (df.select_dtypes(include='number').eq(0).all(axis=1)))] # remove one bad row"
   ],
   "id": "4a537f36205f96c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tables",
   "id": "974ae81f7585c4d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define model groupings for later filtering\n",
    "CLASSICAL_MODELS = [\n",
    "    \"KNeighbors\",\"LogisticRegression\",\"NaiveBayes\",\n",
    "    \"RandomForest\",\"SVM\",\"XGBoost\",\n",
    "]\n",
    "CNN_MODELS = [\"TextCNN42B\"]\n",
    "PLM_MODELS = [\"bert\",\"electra\",\"roberta\",\"xlnet\"]\n",
    "LLM_BASE_MODELS = [\n",
    "    \"3_5_haiku\",\"sonnet_4\",\"4_sonnet\",\n",
    "    \"mistral_small\",\"mistral_large\",\n",
    "    \"gpt_4_1_nano\",\"gpt_4_1_mini\",\"gpt_4_1\",\n",
    "]\n",
    "GPT4_FAMILY = [\"gpt_4_1\",\"gpt_4_1_mini\",\"gpt_4_1_nano\"]\n",
    "\n",
    "ALL_COMPARATORS = (\n",
    "    CLASSICAL_MODELS +\n",
    "    CNN_MODELS +\n",
    "    PLM_MODELS +\n",
    "    LLM_BASE_MODELS\n",
    ")\n",
    "\n",
    "# Table specs for each output table: filter logic and model lists\n",
    "TABLE_SPECS = {\n",
    "    \"table4\": { # All key comparators, representative setting\n",
    "        \"models\" : ALL_COMPARATORS,\n",
    "        \"filters\": dict(\n",
    "            temperature =(0.0, np.nan),\n",
    "            distribution=(\"real\", np.nan),\n",
    "            size        =(2000, np.nan),\n",
    "            prompt_type =(\"default\", np.nan),\n",
    "            vectorizer  =(\"tfidf\",  np.nan),\n",
    "        ),\n",
    "    },\n",
    "\n",
    "    \"table5\": { # All LLMs, representative setting\n",
    "        \"models\" : None,\n",
    "        \"filters\": dict(\n",
    "            category    =\"LLM\",\n",
    "            temperature =0.0,\n",
    "            distribution=(\"real\", np.nan),\n",
    "            size        =(2000, np.nan),\n",
    "            prompt_type =\"default\",\n",
    "        ),\n",
    "    },\n",
    "\n",
    "    \"table6\": { # Only GPT-4 family models, representative setting\n",
    "        \"models\" : GPT4_FAMILY,\n",
    "        \"filters\": dict(\n",
    "            temperature =0.0,\n",
    "            distribution=(\"real\", np.nan),\n",
    "            size        =(2000, np.nan),\n",
    "        ),\n",
    "    },\n",
    "}"
   ],
   "id": "8a36f5476bf9dbfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Function to construct tables based on filters and models",
   "id": "28f369773cac8553"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def make_table(df, models=None, **filters):\n",
    "    \"\"\"\n",
    "    Construct a filtered table based on provided models and column filters.\n",
    "    - models: list or None. If list, keep only those models.\n",
    "    - filters: key-value pairs; if tuple/list, allow any (or NaN).\n",
    "    \"\"\"\n",
    "    mask = pd.Series(True, index=df.index)\n",
    "    if models is not None:\n",
    "        mask &= df[\"model\"].isin(models)\n",
    "    for col, val in filters.items():\n",
    "        if isinstance(val, (list, tuple, set)):\n",
    "            mask &= (df[col].isin(val) | df[col].isna())\n",
    "        else:\n",
    "            mask &= (df[col] == val)\n",
    "\n",
    "    base = [\"model\", \"category\", \"provider\", \"fine_tuned\", \"temperature\",\n",
    "            \"distribution\", \"size\", \"prompt_type\", \"vectorizer\"]\n",
    "    f1s  = [f\"f1_{i}\" for i in range(NUM_LABELS)]\n",
    "    keep = base + f1s + [\"f1_macro\", \"f1_weighted\", \"consistency\"]\n",
    "    keep = [c for c in keep if c in df.columns]\n",
    "    return df.loc[mask, keep].copy()"
   ],
   "id": "2e0f253c62edc9da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Build and export all tables",
   "id": "18bb3c72fd9caa94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tables = {}\n",
    "for name, spec in TABLE_SPECS.items():\n",
    "    tables[name] = make_table(\n",
    "        df,\n",
    "        models  = spec[\"models\"],\n",
    "        **spec[\"filters\"]\n",
    "    )\n",
    "    out_path = os.path.join(VISUAL_DIR, f\"{name}_reviews.xlsx\")\n",
    "    tables[name].to_excel(out_path, index=False)"
   ],
   "id": "fb6ee65c7374b021",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Figures",
   "id": "e6b2cd346d106d11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set global matplotlib visual style for figures\n",
    "plt.rcParams.update({\n",
    "    # Typography\n",
    "    \"font.family\"      : \"Times New Roman\",\n",
    "    \"font.size\"        : 11,\n",
    "    \"axes.titlesize\"   : 14,\n",
    "    \"axes.labelsize\"   : 12,\n",
    "    \"xtick.labelsize\"  : 10,\n",
    "    \"ytick.labelsize\"  : 10,\n",
    "    \"legend.fontsize\"  : 8,\n",
    "    # Grid\n",
    "    \"grid.linestyle\"   : \"--\",\n",
    "    \"grid.linewidth\"   : 0.5,\n",
    "    # Figure saving defaults\n",
    "    \"savefig.dpi\"      : 300,\n",
    "    \"savefig.bbox\"     : \"tight\",\n",
    "})\n",
    "\n",
    "# Define color and marker palettes for LLM and non-LLM models\n",
    "STYLE_LLM = {\n",
    "    \"gpt_4_1\":       {\"color\": \"#08306B\", \"marker\": \"o\", \"label\": \"GPT-4.1\"},\n",
    "    \"gpt_4_1_mini\":  {\"color\": \"#2171B5\", \"marker\": \"s\", \"label\": \"GPT-4.1 Mini\"},\n",
    "    \"gpt_4_1_nano\":  {\"color\": \"#6BAED6\", \"marker\": \"^\", \"label\": \"GPT-4.1 Nano\"},\n",
    "    \"mistral_large\": {\"color\": \"#006D2C\", \"marker\": \"D\", \"label\": \"Mistral Large\"},\n",
    "    \"mistral_small\": {\"color\": \"#74C476\", \"marker\": \"p\", \"label\": \"Mistral Small\"},\n",
    "    \"3_5_haiku\":     {\"color\": \"#A50F15\", \"marker\": \"X\", \"label\": \"Claude Haiku 3.5\"},\n",
    "    \"sonnet_4\":      {\"color\": \"#FB6A4A\", \"marker\": \"*\", \"label\": \"Claude Sonnet 4\"},\n",
    "}\n",
    "STYLE_NON = {\n",
    "    \"RandomForest\":       {\"color\": \"#8B0000\", \"marker\": \"o\", \"label\": \"Random Forest\"},\n",
    "    \"KNeighbors\":         {\"color\": \"#B22222\", \"marker\": \"s\", \"label\": \"K-Nearest Neighbor\"},\n",
    "    \"LogisticRegression\": {\"color\": \"#DC143C\", \"marker\": \"^\", \"label\": \"Logistic Regression\"},\n",
    "    \"NaiveBayes\":         {\"color\": \"#FF6347\", \"marker\": \"D\", \"label\": \"Naïve Bayes\"},\n",
    "    \"SVM\":                {\"color\": \"#FA8072\", \"marker\": \"v\", \"label\": \"SVM\"},\n",
    "    \"XGBoost\":            {\"color\": \"#FF4500\", \"marker\": \"p\", \"label\": \"XGBoost\"},\n",
    "    \"TextCNN42B\":         {\"color\": \"#FFD700\", \"marker\": \"h\", \"label\": \"CNN\"},\n",
    "    \"bert\":               {\"color\": \"#2F4F4F\", \"marker\": \"X\", \"label\": \"BERT\"},\n",
    "    \"electra\":            {\"color\": \"#696969\", \"marker\": \"*\", \"label\": \"ELECTRA\"},\n",
    "    \"roberta\":            {\"color\": \"#A9A9A9\", \"marker\": \"+\", \"label\": \"RoBERTa\"},\n",
    "    \"xlnet\":              {\"color\": \"#D3D3D3\", \"marker\": \"D\", \"label\": \"XLNet\"},\n",
    "}\n",
    "\n",
    "def build_legend_handles(style_dict, dashed_note=False):\n",
    "    \"\"\"\n",
    "    Build legend handles for consistent publication figures.\n",
    "    \"\"\"\n",
    "    handles = [\n",
    "        Line2D([0],[0], color=p[\"color\"], marker=p[\"marker\"],\n",
    "               linestyle=\"-\", linewidth=1, markersize=6)\n",
    "        for p in style_dict.values()\n",
    "    ]\n",
    "    labels = [p[\"label\"] for p in style_dict.values()]\n",
    "    if dashed_note:\n",
    "        handles.append(Line2D([0],[0], color=\"black\", linestyle=\"--\", linewidth=1))\n",
    "        labels.append(\"Fine-Tuned Versions\")\n",
    "    return handles, labels"
   ],
   "id": "baa68c2d92db2b3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Functions to plot temperature, data size, and data distribution variations",
   "id": "1f0f3536385b59b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot performance curves over temperature for LLMs\n",
    "def plot_llm_temp(df, metric, ylabel, title, filename, ylim):\n",
    "    \"\"\"\n",
    "    Plot LLM results as a function of temperature.\n",
    "    Used for Figure 6 (main paper).\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    for (model, ft), grp in df.groupby([\"model\", \"fine_tuned\"]):\n",
    "        props = STYLE_LLM[model]\n",
    "        ls    = \"--\" if ft else \"-\"\n",
    "        grp   = grp.sort_values(\"temperature\")\n",
    "        ax.plot(grp[\"temperature\"], grp[metric],\n",
    "                color=props[\"color\"], marker=props[\"marker\"],\n",
    "                linestyle=ls, linewidth=1, markersize=6)\n",
    "    handles, labels = build_legend_handles(STYLE_LLM, dashed_note=True)\n",
    "    ax.legend(handles, labels, loc=\"lower left\",\n",
    "              frameon=True, framealpha=.7, edgecolor=\"black\")\n",
    "    ax.set(xlabel=\"Temperature\", ylabel=ylabel, title=title,\n",
    "           xticks=[0, 0.5, 1, 1.5], ylim=ylim)\n",
    "    ax.grid(True)\n",
    "    fig.savefig(os.path.join(VISUAL_DIR, filename))\n",
    "    plt.show()\n",
    "\n",
    "# Plot F1 vs. data size for both non-LLM and LLM models\n",
    "def plot_f1_vs_size(df_dist, df_base_llm, xticks, dist_label):\n",
    "    \"\"\"\n",
    "    Plot F1-score as a function of training data size for\n",
    "    (a) ML/CNN/PLM and (b) LLMs, for a given distribution.\n",
    "    Used for Plots Figures 7 (main paper) and C1 (Online Appendix)\n",
    "    \"\"\"\n",
    "    # non-LLM slice (tfidf only)\n",
    "    df_nonllm = df_dist[\n",
    "        (df_dist[\"category\"] != \"LLM\") &\n",
    "        ((df_dist[\"vectorizer\"] == \"tfidf\") | df_dist[\"vectorizer\"].isna())\n",
    "    ].copy()\n",
    "    if not df_nonllm.empty:\n",
    "        fig, ax = plt.subplots(figsize=(8,5))\n",
    "        for m, grp in df_nonllm.groupby(\"model\"):\n",
    "            props = STYLE_NON[m]\n",
    "            grp   = grp.sort_values(\"size\")\n",
    "            ax.plot(grp[\"size\"], grp[\"f1_macro\"],\n",
    "                    color=props[\"color\"], marker=props[\"marker\"],\n",
    "                    linestyle=\"-\", linewidth=1)\n",
    "        h, l = build_legend_handles(STYLE_NON)\n",
    "        ax.legend(h, l, loc=\"lower right\", frameon=True, framealpha=.7)\n",
    "        ax.set(xlabel=\"Training-Data Size (N)\", ylabel=\"Macro Avg. F1-Score\",\n",
    "               title=f\"Demonstration Study 2: Macro Avg. F1-Score vs. Size\\n({dist_label} Distribution, non-LLM)\",\n",
    "               xticks=xticks, ylim=(0, 0.5))\n",
    "        ax.grid(True); plt.tight_layout()\n",
    "        fig.savefig(os.path.join(VISUAL_DIR, f\"figure_C1_nonLLM_f1-size-{dist}_reviews.png\"))\n",
    "        plt.show()\n",
    "\n",
    "    # LLM slice (base point + dashed fine-tune curve)\n",
    "    df_llm_ft = df_dist[\n",
    "        (df_dist[\"category\"] == \"LLM\") &\n",
    "        (df_dist[\"fine_tuned\"] != 0)\n",
    "    ].copy()\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    x0, xmax = 0, xticks[-1]\n",
    "\n",
    "    ## Plot base LLMs as horizontal lines from x=0 (default) to max\n",
    "    for m, row in df_base_llm.groupby(\"model\"):\n",
    "        props = STYLE_LLM[m]\n",
    "        f1    = row[\"f1_macro\"].iloc[0]\n",
    "        ax.scatter(x0, f1, color=props[\"color\"], marker=props[\"marker\"], s=36, zorder=3)\n",
    "        ax.hlines(f1, x0, xmax, color=props[\"color\"], linewidth=1, zorder=2)\n",
    "\n",
    "    ## Dashed curves for fine-tuned LLMs\n",
    "    for m, grp in df_llm_ft.groupby(\"model\"):\n",
    "        props = STYLE_LLM[m]\n",
    "        grp   = grp.sort_values(\"size\")\n",
    "        ax.plot(grp[\"size\"], grp[\"f1_macro\"],\n",
    "                color=props[\"color\"], marker=props[\"marker\"],\n",
    "                linestyle=\"--\", linewidth=1, markersize=6)\n",
    "    h, l = build_legend_handles(STYLE_LLM, dashed_note=True)\n",
    "    xtick_full = [0] + xticks\n",
    "    ax.legend(h, l, loc=\"lower right\", frameon=True, framealpha=.7)\n",
    "    ax.set(xlabel=\"Training-Data Size (N)\", ylabel=\"Macro Avg. F1-Score\",\n",
    "           title=f\"Demonstration Study 2: Macro Avg. F1-Score vs. Size\\n({dist_label} Distribution, LLM)\",\n",
    "           xticks=xtick_full,\n",
    "           xticklabels=['0\\n(default)'] + list(map(str, xticks)),\n",
    "           ylim=(0.2, 0.8))\n",
    "    ax.grid(True); plt.tight_layout()\n",
    "    fig.savefig(os.path.join(VISUAL_DIR,f\"figure_7_LLM_f1-size-{dist}_reviews.png\"))\n",
    "    plt.show()"
   ],
   "id": "2bf89a5268eb58a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plot and export all figures",
   "id": "fca9009a855b5b8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# For temperature plots (LLM only)\n",
    "wanted_llm = list(STYLE_LLM.keys())\n",
    "df_plot = df[\n",
    "    (df[\"model\"].isin(wanted_llm)) &\n",
    "    ((df[\"distribution\"] == \"real\") | df[\"distribution\"].isna()) &\n",
    "    ((df[\"size\"] == 2000) | df[\"size\"].isna()) &\n",
    "    (df[\"prompt_type\"] == \"default\")\n",
    "].copy()\n",
    "\n",
    "# For data size plots: combine ML/CNN/PLM with LLMs\n",
    "df_mlcnnplm = df[df[\"category\"] != \"LLM\"]\n",
    "df_llm_sel  = df[(df[\"category\"] == \"LLM\") & (df[\"model\"].isin(wanted_llm))]\n",
    "df_combined = pd.concat([df_mlcnnplm, df_llm_sel], ignore_index=True)\n",
    "\n",
    "# Conditioned on default runs\n",
    "df_cond = df_combined[\n",
    "    ((df_combined[\"prompt_type\"] == \"default\") | df_combined[\"prompt_type\"].isna()) &\n",
    "    ((df_combined[\"temperature\"] == 0.0) | df_combined[\"temperature\"].isna()) &\n",
    "    ((df_combined[\"vectorizer\"] == \"tfidf\") | df_combined[\"vectorizer\"].isna())\n",
    "].copy()"
   ],
   "id": "9bb0f167cbd9549f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Temperature curves: Figure 6 (F1, Consistency vs. Temperature)\n",
    "plot_llm_temp(\n",
    "    df_plot,\n",
    "    metric   =\"f1_macro\",\n",
    "    ylabel   =\"Macro Avg. F1-Score\",\n",
    "    title    =\"Demonstration Study 2: Macro Avg. F1-Score and Temperature\",\n",
    "    filename =\"figure_6_LLM_f1-temperature_reviews.png\",\n",
    "    ylim     =(0.25, 0.80),\n",
    ")\n",
    "plot_llm_temp(\n",
    "    df_plot,\n",
    "    metric   =\"consistency\",\n",
    "    ylabel   =\"Consistency Rate\",\n",
    "    title    =\"Demonstration Study 2: Consistency Rate and Temperature\",\n",
    "    filename =\"figure_6_LLM_consistency-temperature_reviews.png\",\n",
    "    ylim     =(0.60, 1.00),\n",
    ")\n",
    "\n",
    "# F1 vs size: Figures 7 and C1 (LLM and non-LLM)\n",
    "label_map = {\"real\": \"Representative\", \"equal\": \"Balanced\"}\n",
    "xticks = [100, 250, 500, 1000, 2000]\n",
    "\n",
    "for dist in [\"real\", \"equal\"]:\n",
    "    dist_lbl = label_map[dist]\n",
    "    df_dist = df_cond[\n",
    "        (df_cond[\"distribution\"] == dist) & df_cond[\"size\"].notna()\n",
    "    ].copy()\n",
    "\n",
    "    df_base_llm = df_cond[\n",
    "        (df_cond[\"category\"] == \"LLM\") &\n",
    "        (df_cond[\"fine_tuned\"] == 0) &\n",
    "        (df_cond[\"model\"].isin(STYLE_LLM.keys()))\n",
    "    ].copy()\n",
    "    plot_f1_vs_size(df_dist, df_base_llm, xticks, dist_lbl)"
   ],
   "id": "96c4b9c1de16eb51",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
